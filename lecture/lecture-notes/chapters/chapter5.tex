\chapter{Resolution Method}\label{chapter:propositional-resolution}

In this chapter, we introduce another proof system more suitable for practical applications, called the \emph{resolution method}. This method is the foundation of, among others, \emph{logical programming}, \emph{automated theorem proving}, or \emph{software verification}. In this chapter, we limit ourselves to the resolution method in propositional logic, but later, in Chapter~\ref{chapter:predicate-resolution}, we will introduce the concept of \emph{unification}, which allows us to search for resolution proofs in predicate logic.

The resolution method works with propositions in \emph{conjunctive normal form (CNF)}. Recall that every proposition can be converted to CNF. This conversion is, in the worst case, of exponential time complexity (there are even propositions whose shortest CNF equivalent is exponentially longer), but in practice, this is not an issue.

Similar to the tableau method, the resolution method is based on proof by contradiction, i.e., we add the \emph{negation} of the proposition we want to prove to the theory in which we are proving (both converted to CNF), and show that this leads to a contradiction.

To find the contradiction, the resolution method uses a single inference rule called the \emph{resolution rule}. This is a special case of the \emph{cut rule}, which states: \emph{``From the propositions $\varphi \lor \psi$ and $\neg \varphi \lor \chi$, we can infer the proposition $\psi \lor \chi$,''} written as:
$$
\frac{\varphi \lor \psi, \neg \varphi \lor \chi}{\psi \lor \chi}
$$
In the \emph{resolution rule}, which we will demonstrate shortly, $\varphi$ is a \emph{literal}, and $\psi, \chi$ are \emph{clauses}.

\begin{exercise}
Show that the cut rule is \emph{sound}. (What does it mean?)
\end{exercise}

\section{Set Representation}

First, we introduce a more compact notation for CNF propositions, called \emph{set notation}. For it would be impractical to represent CNF propositions as strings including brackets and logical symbols.
\begin{itemize}
    \item Recall that a \emph{literal} $\ell$ is a propositional variable or its negation, and that $\bar{\ell}$ denotes the \emph{opposite literal} to $\ell$.
    \item A \emph{clause} $C$ is a finite set of literals. The \emph{empty clause}, which is never satisfied,\footnote{It represents the disjunction of an empty set of literals, meaning none of the disjuncts are satisfied.} is denoted by $\square$.
    \item A \emph{(CNF) formula} $S$ is a (finite, or even infinite) set of clauses. The \emph{empty formula} $\emptyset$ is always satisfied.\footnote{It represents the conjunction of an empty set of clauses, meaning all clauses in $S$ are satisfied.}
\end{itemize}

\begin{remark}
    Note that a \emph{CNF formula} can also be an \emph{infinite} set of clauses. If we convert an infinite propositional theory to CNF, we take (in set representation) all infinitely many clauses as elements of a single formula (set). In practical applications, the formula is (almost always) finite.
\end{remark}

In set notation, models correspond to sets of literals that contain exactly one of the literals $p, \neg p$ for each propositional variable $p$:
\begin{itemize}
    \item A \emph{(partial) assignment $\mathcal{V}$} is any set of literals that is \emph{consistent}, i.e., it does not contain both a literal and its negation.
    \item An assignment is \emph{complete} if it contains either the positive or negative literal for each propositional variable.
    \item An assignment $\mathcal{V}$ \emph{satisfies} a formula $S$, written $\mathcal{V} \models S$, if $\mathcal{V}$ contains some literal from each clause in $S$, i.e.,
    $$
    \mathcal{V} \cap C \neq \emptyset \text{ for each } C \in S.
    $$
\end{itemize}

\begin{example}\label{example:set-representation}
    The proposition $\varphi = (\neg p_1 \lor p_2) \land (\neg p_1 \lor \neg p_2 \lor p_3) \land (\neg p_3 \lor \neg p_4) \land (\neg p_4 \lor \neg p_5) \land p_4$ is written in set notation as:
    $$
    S = \{\{\neg p_1, p_2\}, \{\neg p_1, \neg p_2, p_3\}, \{\neg p_3, \neg p_4\}, \{\neg p_4, \neg p_5\}, \{p_4\}\}
    $$
    The assignment $\mathcal{V} = \{\neg p_1, \neg p_3, p_4, \neg p_5\}$ satisfies $S$, written $\mathcal{V} \models S$. It is not complete, but we can extend it with any literal for $p_2$: both $\mathcal{V} \cup \{p_2\} \models S$ and $\mathcal{V} \cup \{\neg p_2\} \models S$ hold. These two complete assignments correspond to the models $(0,1,0,1,0)$ and $(0,0,0,1,0)$.
\end{example}

\section{Resolution Proof}

First, we define one inference step in the resolution proof, the \emph{resolution rule}, which we apply to a pair of clauses; the result is a clause called the \emph{resolvent}, which is a logical consequence of the original pair of clauses:

\begin{definition}[Resolution Rule]
    Given clauses $C_1$ and $C_2$ and a literal $\ell$ such that $\ell \in C_1$ and $\bar{\ell} \in C_2$, the \emph{resolvent} of the clauses $C_1$ and $C_2$ \emph{over the literal} $\ell$ is the clause
    $$
    C = (C_1 \setminus \{\ell\}) \cup (C_2 \setminus \{\bar{\ell}\}).
    $$
\end{definition}

So, we remove the literal $\ell$ from the first clause and the literal $\bar{\ell}$ from the second clause (both had to be there!) and take the union of all remaining literals to be the resulting resolvent. Using the symbol $\disjointunion$ for disjoint union, we could also write:
$$
C_1' \cup C_2' \text{ is the resolvent of the clauses } C_1' \disjointunion \{\ell\} \text{ and } C_2' \disjointunion \{\bar{\ell}\}
$$

\begin{example}
    From the clauses $C_1 = \{\neg q, r\}$ and $C_2 = \{\neg p, \neg q, \neg r\}$, we can derive the resolvent $\{\neg p, \neg q\}$ over the literal $r$. From the clauses $\{p, q\}$ and $\{\neg p, \neg q\}$, we can derive $\{p, \neg p\}$ over the literal $q$ or $\{q, \neg q\}$ over the literal $p$ (both of which are tautologies).\footnote{We cannot, however, derive $\square$ `by resolving over $p$ and $q$ simultaneously' (a common mistake). Note that $\{\{p, q\}, \{\neg p, \neg q\}\}$ is not unsatisfiable, e.g., $(1,0)$ is a model.}
\end{example}

\begin{observation}[Soundness of the Resolution Rule]
The resolution rule is sound, i.e., for any assignment $\mathcal{V}$, the following holds:
$$
\text{If } \mathcal{V} \models C_1 \text{ and } \mathcal{V} \models C_2, \text{ then } \mathcal{V} \models C.
$$
\end{observation}

A resolution proof is defined similarly to a Hilbert proof as a finite sequence of clauses, ensuring the validity of each clause in the sequence: at each step, we can either write an `axiom' (a clause from $S$) or a resolvent of some two previously written clauses.

\begin{definition}[Resolution Proof]
    A \emph{resolution proof} of a clause $C$ from a CNF formula $S$ is a \emph{finite} sequence of clauses $C_0, C_1, \dots, C_n = C$ such that for each $i$, either $C_i \in S$ or $C_i$ is the resolvent of some $C_j, C_k$ where $j < i$ and $k < i$.

    If a resolution proof exists, we say that $C$ is \emph{resolution provable} from $S$, written $S \proves_R C$. A \emph{(resolution) refutation} of the CNF formula $S$ is a resolution proof of $\square$ from $S$, in which case $S$ is \emph{(resolution) refutable}.
\end{definition}

\begin{example}\label{example:resolution-proof}
    The CNF formula $S = \{\{p, \neg q, r\}, \{p, \neg r\}, \{\neg p, r\}, \{\neg p, \neg r\}, \{q, r\}\}$ is (resolution) refutable, one possible refutation is:
    $$
    \{p, \neg q, r\}, \{q, r\}, \{p, r\}, \{\neg p, r\}, \{r\}, \{p, \neg r\}, \{\neg p, \neg r\}, \{\neg r\}, \square
    $$
\end{example}

A resolution proof has a natural tree structure: the leaves are labeled by axioms and the inner nodes represent individual resolution steps.

\begin{definition}[Resolution Tree]
A resolution tree of a clause $C$ from a CNF formula $S$ is a \emph{finite} binary tree with nodes labeled by clauses, where:
\begin{itemize}
    \item The root is labeled $C$,
    \item The leaves are labeled by clauses from $S$,
    \item Each inner node is labeled by a resolvent of its child nodes.
\end{itemize}    
\end{definition}

\begin{example}\label{example:resolution-tree}
The resolution tree of the empty clause $\square$ from the CNF formula $S$ from Example \ref{example:resolution-proof} is:
\begin{center}
    \begin{forest}
    for tree={grow=north}
    [$ \square $
        [$ \{\neg r\} $
            [{$ \{\neg p, \neg r\} $}]
            [{$ \{p, \neg r\} $}]
        ]
        [$ \{r\} $
            [{$ \{\neg p, r\} $}]
            [{$ \{p,r\} $}
                [{$ \{q, r\} $}]
                [{$ \{p, \neg q, r\} $}]
            ]
        ]
    ]
    \end{forest}
\end{center}
\end{example}

It is easy to prove the following observation, by induction on the depth of the tree and the length of the resolution proof:

\begin{observation} A clause $C$ has a resolution tree from a CNF formula $S$ if and only if $S \proves_R C$.  
\end{observation}


%todo


Each resolution proof corresponds to a unique resolution tree. Conversely, from a single resolution tree, we can derive multiple resolution proofs: they are given by any traversal of the tree nodes where an inner node is visited only after both of its children have been visited.

We also introduce another concept, called the \emph{resolution closure}, which contains all clauses that can be `learned' by resolution from a given formula. It is a useful theoretical perspective on resolution; in applications, constructing the entire resolution closure would be impractical.

\begin{definition}[Resolution Closure]
The \emph{resolution closure} $\mathcal{R}(S)$ of the formula $S$ is defined inductively as the smallest set of clauses satisfying:
\begin{itemize}
    \item $C \in \mathcal{R}(S)$ for all $C \in S$,
    \item If $C_1, C_2 \in \mathcal{R}(S)$ and $C$ is the resolvent of $C_1, C_2$, then $C \in \mathcal{R}(S)$.
\end{itemize}
\end{definition}

\begin{example}
    Let us compute the resolution closure of the formula $S = \{\{p, \neg q, r\}, \{p, \neg r\}, \{\neg p, r\}, \{\neg p, \neg r\}, \{q, r\}\}$. The clauses from $S$ are in \textcolor{blue}{blue}; additional clauses are obtained by resolving them in pairs (first with the first, second with the first, second with the second, etc., over all possible literals):
    \begin{align*}
        \mathcal{R}(S) = \{&\textcolor{blue}{\{p, \neg q, r\}, \{p, \neg r\}, \{\neg p, r\}, \{\neg p, \neg r\}, \{q, r\}},\\ &\{p, \neg q\}, \{\neg q, r\}, \{r, \neg r\}, \{p, \neg p\}, \{r, \neg s\}, \{p, r\}, \{p, q\}, \{r\}, \{p\}\}
    \end{align*}
\end{example}

\section{Soundness and Completeness of the Resolution Method}

The resolution method is also both sound and complete. 

\subsection{Soundness of Resolution}

Soundness can be easily proved by induction on the length of the resolution proof.

\begin{theorem}[Soundness of Resolution]\label{theorem:soundness-resolution}
If a formula $S$ is resolution refutable, then $S$ is unsatisfiable.
\end{theorem}
\begin{proof}
    Suppose $S \proves_R \square$ and consider a resolution proof $C_0, C_1, \dots, C_n = \square$. Suppose, for a contradiction, that $S$ is satisfiable, i.e., $\mathcal{V} \models S$ for some assignment $\mathcal{V}$. By induction on $i$, we prove that $\mathcal{V} \models C_i$. For $i = 0$, this holds because $C_0 \in S$. For $i > 0$, there are two cases:
    \begin{itemize}
        \item $C_i \in S$, in which case $\mathcal{V} \models C_i$ follows from the assumption that $\mathcal{V} \models S$,
        \item $C_i$ is the resolvent of $C_j, C_k$, where $j, k < i$: by the induction hypothesis, $\mathcal{V} \models C_j$ and $\mathcal{V} \models C_k$. $\mathcal{V} \models C_i$ follows from the soundness of the resolution rule.
    \end{itemize}
    (Alternatively, we could proceed by induction on the depth of the resolution tree.)
\end{proof}

\subsection{Tree of Reductions}

In the completeness proof, we need to construct a resolution tree, whose construction is based on the so-called \emph{tree of reductions}. By \emph{substituting} a literal into a formula, we mean simplifying the formula under the assumption that the given literal is true. Substitution was already encountered in Section \ref{section:horn-sat} during \emph{unit propagation}: we remove clauses containing this literal and remove the opposite literal from the remaining clauses.

\begin{definition}[Literal Substitution]
    Given a formula $S$ and a literal $\ell$, the \emph{substitution} of $\ell$ into $S$ is the formula:
    $$
        S^\ell = \{C \setminus \{\bar{\ell}\} \mid \ell \notin C \in S\}
    $$    
\end{definition}

\begin{observation} Here we summarize several simple facts about substitution:
\begin{itemize}
    \item $S^\ell$ is the result of \emph{unit propagation} applied to $S \cup \{\{\ell\}\}$.
    \item $S^\ell$ does not contain the literal $\ell$ or its opposite $\bar{\ell}$ (it does not contain the propositional variable from $\ell$ at all).
    \item If $S$ did not contain the literal $\ell$ or its opposite $\bar{\ell}$, then $S^\ell = S$.
    \item If $S$ contained the unit clause $\{\bar{\ell}\}$, then $\square \in S^\ell$, meaning $S^\ell$ is unsatisfiable.
\end{itemize}    
\end{observation}

The key property of substitution is expressed by the following lemma:

\begin{lemma}\label{lemma:tree-of-reductions}
$S$ is satisfiable if and only if $S^\ell$ or $S^{\bar{\ell}}$ is satisfiable.    
\end{lemma}
\begin{proof}
Let $\mathcal{V} \models S$, it cannot contain both $\ell$ and $\bar{\ell}$ (it must be consistent); without loss of generality, assume $\bar{\ell} \notin \mathcal{V}$, and show that $\mathcal{V} \models S^\ell$. Consider any clause in $S^\ell$. It is of the form $C \setminus \{\bar{\ell}\}$ for a clause $C \in S$ (not containing the literal $\ell$). We know that $\mathcal{V} \models C$, but since $\mathcal{V}$ does not contain $\bar{\ell}$, the assignment $\mathcal{V}$ must satisfy some other literal of $C$, so $\mathcal{V} \models C \setminus \{\bar{\ell}\}$.

Conversely, suppose there exists an assignment $\mathcal{V}$ satisfying $S^\ell$ (again, without loss of generality). Since $\bar{\ell}$ (or $\ell$) does not appear in $S^\ell$, it holds that $\mathcal{V} \setminus \{\bar{\ell}\} \models S^\ell$. The assignment $\mathcal{V}' = (\mathcal{V} \setminus \{\bar{\ell}\}) \cup \{\ell\}$ then satisfies each clause $C \in S$: if $\ell \in C$, then $\ell \in C \cap \mathcal{V}'$ and $C \cap \mathcal{V}' \neq \emptyset$, otherwise $C \cap \mathcal{V}' = (C \setminus \{\bar{\ell}\}) \cap \mathcal{V}' \neq \emptyset$ because $\mathcal{V} \setminus \{\bar{\ell}\} \models C \setminus \{\bar{\ell}\} \in S^\ell$. We have verified that $\mathcal{V}' \models S$, so $S$ is satisfiable.
\end{proof}

Whether a given \emph{finite} formula is satisfiable can thus be determined recursively (using the \emph{divide and conquer} method) by substituting both possible literals for (some, say the first) propositional variable appearing in the formula and branching the computation. Essentially, this is a similar principle to the DPLL algorithm (see Section \ref{section:DPLL}). The resulting tree is called a \emph{tree of reductions}. 

\begin{example}
Let's illustrate this concept with an example by constructing a tree of reductions for the formula $S = \{\{p\}, \{\neg q\}, \{\neg p, \neg q\}\}$:
\begin{center}
    \begin{forest}    
    [{$S$}
        [{$S^p = \{\{\neg q\}\}$}
            [{$S^{pq} = \{\square\}$}, tikz={\node[fit to=tree,label=below:\textcolor{red}{$\otimes$}] {};]}]
            [{$S^{p\bar{q}} = \emptyset$}, tikz={\node[fit to=tree,label=below:\textcolor{blue}{\small $\mathcal{V} = \{p, \bar{q}\}$}] {};]}]
        ]
        [{$S^{\bar{p}} = \{\square, \{\neg q\}\}$}, tikz={\node[fit to=tree,label=below:\textcolor{red}{$\otimes$}] {};]}]
    ]
    \end{forest}
\end{center}
Once a branch contains the empty clause $\square$, it is unsatisfiable, and we need not continue in that branch. In the leaves, there are either unsatisfiable theories or empty theories: in the latter case, the sequence of substitutions gives us a satisfying assignment.    
\end{example}

From the construction, it is evident how to proceed in the case of a finite formula. However, the tree of reductions makes sense, and the following corollary holds, even for infinite formulas:

\begin{corollary}
    A formula $S$ (over a countable language) is unsatisfiable if and only if every branch of the tree of reductions contains the empty clause $\square$.
\end{corollary}

\begin{proof}
    For a finite formula $S$, this follows from the above discussion; it can be easily proved by induction on the size of $\Var(S)$: 
    \begin{itemize}
        \item If $|\Var(S)| = 0$, we have $S = \emptyset$ or $S = \{\square\}$, in both cases, the tree of reductions is one node, and the statement holds. 
        \item In the inductive step, we choose any literal $\ell \in \Var(S)$ and apply Lemma \ref{lemma:tree-of-reductions}.
    \end{itemize} 
If $S$ is infinite and satisfiable, it has a satisfying assignment, which `matches' the corresponding (infinite) branch in the tree of reductions. If $S$ is infinite and unsatisfiable, then by the Compactness Theorem, there is a finite part $S' \subseteq S$ that is also unsatisfiable. Substitution for all variables from $\Var(S')$ will lead to $\square$ in each branch, which will happen after finitely many steps.
\end{proof}

\subsection{Completeness of Resolution}

\begin{theorem}[Completeness of Resolution]
If $S$ is unsatisfiable, it is resolution refutable (i.e., $S \proves_R \square$).
\end{theorem}   
\begin{proof}
If $S$ is infinite, it has a finite unsatisfiable part $S'$ by the Compactness Theorem. A resolution refutation of $S'$ is also a resolution refutation of $S$. Suppose $S$ is finite.

The proof is by induction on the number of variables in $S$. If $|\Var(S)| = 0$, the only possible unsatisfiable formula without variables is $S = \{\square\}$, and we have a one-step proof $S \proves_R \square$. Otherwise, choose $p \in \Var(S)$. By Lemma \ref{lemma:tree-of-reductions}, both $S^p$ and $S^{\bar{p}}$ are unsatisfiable. They have one fewer variable, so by the induction hypothesis, there are resolution trees $T$ for $S^p \proves_R \square$ and $T'$ for $S^{\bar{p}} \proves_R \square$.

We show how to construct the resolution tree $\widehat{T}$ for $S \proves_R \neg p$. Similarly, we construct $\widehat{T'}$ for $S \proves_R p$, and then easily construct the resolution tree for $S \proves_R \square$: we attach the roots of the trees $\widehat{T}$ and $\widehat{T'}$ as the left and right children of the root $\square$ (i.e., in the last step of the resolution proof, we obtain $\square$ by resolving $\{\neg p\}$ and $\{p\}$).

It remains to show the construction of the tree $\widehat{T}$: the set of nodes and the order remain the same; we only change some clauses in the nodes by adding the literal $\neg p$. At each leaf of the tree $T$ is some clause $C \in S^p$, and either $C \in S$ or it is not, but $C \cup \{\neg p\} \in S$. In the first case, we leave the label unchanged. In the second case, we add the literal $\neg p$ to $C$ \emph{and to all clauses above this leaf}. In the leaves, there are now only clauses from $S$, in the root, we have changed $\square$ to $\neg p$. And each inner node remains the resolvent of its children.
\end{proof}

\begin{exercise}
    The proof of the Completeness Theorem for resolution gives a method for recursively `growing' a resolution refutation. Think about how to do this and apply it to an example of an unsatisfiable formula.
\end{exercise}


\section{LI-Resolution and Horn-SAT}

We begin with a different view of the resolution proof, called the \emph{linear proof}.

\subsection{Linear Proof}

In addition to the resolution tree, a resolution proof can also be organized as a \emph{linear proof}, where in each step we have one \emph{central} clause, which we resolve with a \emph{side} clause, which is either one of the previous central clauses or an axiom from $S$. The resolvent then becomes the new central clause.\footnote{While the construction of the resolution tree can be easily described recursively, the linear proof better corresponds to a procedural computation. It is only about finding a suitable side clause.}

\begin{definition}[Linear Proof]
    A \emph{linear proof} (by resolution) of a clause $C$ from a formula $S$ is a finite sequence
    $$
    \begin{bmatrix}
        C_0 \\
        B_0
    \end{bmatrix},
    \begin{bmatrix}
        C_1 \\
        B_1
    \end{bmatrix},\dots,
    \begin{bmatrix}
        C_n \\
        B_n
    \end{bmatrix},
    C_{n+1}
    $$
    where $C_i$ are called \emph{central} clauses, $C_0$ is the \emph{initial} clause, $C_{n+1}=C$ is the \emph{final} clause, $B_i$ are \emph{side} clauses, and it holds that:
    \begin{itemize}
        \item $C_0\in S$, for $i\leq n$, $C_{i+1}$ is the resolvent of $C_i$ and $B_i$,
        \item $B_0\in S$, for $i\leq n$, $B_i\in S$ or $B_i=C_j$ for some $j<i$. 
    \end{itemize}
    A \emph{linear refutation} of $S$ is a linear proof of $\square$ from $S$. A linear proof can be illustrated as follows:
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex}
            [C_{n+1}
                [,phantom]
                [C_n
                    [,phantom]
                    [\cdots\cdots\cdots
                        [C_2
                            [,phantom]
                            [C_1
                                [,phantom]
                                [C_0]
                                [B_0]
                            ]
                            [B_1]
                        ]
                    ]
                    [B_{n-1}]                    
                ]
                [B_n]
            ]
        \end{forest}  
    \end{center}
\end{definition}

\begin{example} \label{example:linear-proof}
    Let's construct a linear refutation of the formula $S = \{\{p, q\},\{p, \neg q\}, \{\neg p, q\}, \{\neg p, \neg q\}\}$ (i.e., a linear proof of $\square$ from $S$). A linear proof might look like this:

    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
            [{\square}
                [,phantom]
                [{\{\neg p\}}
                    [,phantom]
                    [{\{q\}}
                        [,phantom]
                        [{\textcolor{blue}{\{p\}}}
                            [,phantom]
                            [{\{p,q\}}]
                            [{\{p,\neg q\}}]
                        ]
                        [{\{\neg p,q\}}]
                    ]
                    [{\{\neg p,\neg q\}}]                    
                ]
                [{\textcolor{red}{\{p\}}}]
            ]
        \end{forest}  
    \end{center}
    The last side clause $\textcolor{red}{\{p\}}$ (in red) is not from $S$, but is equal to the previous central clause (in blue).
\end{example}

\begin{exercise}
    Convert the linear proof from Example \ref{example:linear-proof} into a resolution tree.
\end{exercise}

\begin{remark}\label{remark:linear-resolution}
    $C$ has a linear proof from $S$ if and only if $S\proves_R C$.
\end{remark}
A resolution tree can easily be produced from a linear proof by induction on the length of the proof: the base case is obvious, and if there is a side clause $B_i$ that is not an axiom from $S$, then $B_i=C_j$ for some $j<i$ and we only need to attach the resolution tree for proving $C_j$ from $S$ instead of $B_i$. Notice that this also implies the \emph{soundness} of linear resolution.

We will not present the proof of the reverse implication. It follows from the \emph{completeness} of linear resolution, whose proof can be found in the textbook \emph{A. Nerode, R. Shore: Logic for Applications}~\cite{nerode_logic_2012}.


\subsection{LI-Resolution}

In a general linear proof, each subsequent side clause can either be an axiom from $S$ or one of the previous central clauses. If we forbid the latter option and require that all side clauses must be from $S$, we get the so-called \emph{LI (linear-input) resolution}:
    
\begin{definition}[LI-Proof]
    An \emph{LI-proof} (by resolution) of a clause $C$ from a formula $S$ is a linear proof 
    $$
    \begin{bmatrix}
        C_0 \\
        B_0
    \end{bmatrix},
    \begin{bmatrix}
        C_1 \\
        B_1
    \end{bmatrix},\dots,
    \begin{bmatrix}
        C_n \\
        B_n
    \end{bmatrix},
    C
    $$
    in which each side clause $B_i$ is an axiom from $S$. If an LI-proof exists, we say that $C$ is \emph{LI-provable} from $S$, and we write $S\proves_{LI}C$. If $S\proves_{LI}\square$, then $S$ is \emph{LI-refutable}.
\end{definition}

\begin{remark}
    An LI-proof directly gives a resolution tree (all leaves are axioms) in a special form that we might call a `hairy path'. Conversely, from a resolution tree in the form of a hairy path, we immediately obtain an LI-proof: the vertices on the path are central clauses, the hairs are side clauses.
\end{remark}

While \emph{linear resolution}\footnote{That is, a proof system based on finding \emph{linear} proofs or refutations.} is just another view of the general resolution proof, \emph{LI-resolution} introduces a significant restriction: we lose \emph{completeness} (not every unsatisfiable formula has an LI-refutation). On the other hand, LI-proofs are simpler to construct.\footnote{In each step, we only have to choose from clauses in $S$, not from previously proven central clauses.} 

\subsection{Completeness of LI-Resolution for Horn Formulas}

As we will now show, LI-resolution is \emph{complete for Horn formulas}. As we will see in the next section, it is the basis of Prolog interpreters, which work with Horn formulas. First, let us recall the terminology related to hornness and also programs, in set representation:

\begin{itemize}
    \item A \emph{Horn clause} is a clause containing at most one positive literal.
    \item A \emph{Horn formula} is (finite or even infinite) a set of Horn clauses.
    \item A \emph{fact} is a positive unit (Horn) clause, i.e., $\{p\}$, where $p`$ is a propositional variable.
    \item A \emph{rule} is a (Horn) clause with exactly one positive and at least one negative literal.
    \item Rules and facts are called \emph{program clauses}.
    \item A \emph{goal} is a non-empty (Horn) clause without a positive literal.\footnote{Recall that we prove by \emph{contradiction}, so the \emph{goal} is the negation of what we want to prove.}
\end{itemize}

We will find the following simple observation useful:

\begin{observation}\label{observation:horn-fact-goal}
    If a Horn formula $S$ is unsatisfiable and $\square\notin S$, then it contains a fact and a goal.
\end{observation}
\begin{proof}
    If it does not contain a fact, we can evaluate all variables to 0; if it does not contain a goal, we evaluate to 1.
\end{proof}

Now we will state and prove the Completeness Theorem for LI-Resolution for Horn formulas. The proof also provides a guide on how to construct an LI-refutation, based on the process of unit propagation. This procedure is illustrated in the example below, which you can follow along with the reading of the proof.

\begin{theorem}[Completeness of LI-Resolution for Horn Formulas]\label{theorem:completeness-of-li-resolution-for-horn}
If a Horn formula $T$ is satisfiable, and $T\cup\{G\}$ is unsatisfiable for a goal $G$, then $T\cup\{G\}\proves_{LI}\square$, by an LI-refutation that starts with the goal $G$.   
\end{theorem}
\begin{proof}
    Similarly to the Completeness Theorem for Resolution, we can assume by the Compactness Theorem that $T$ is finite. The proof (construction of the LI-refutation) will be done by induction on the number of variables in $T$.

    By Observation \ref{observation:horn-fact-goal}, $T$ contains a fact $\{p\}$ for some propositional variable $p$. Since $T\cup\{G\}$ is unsatisfiable, according to Lemma \ref{lemma:tree-of-reductions}, $(T\cup\{G\})^p=T^p\cup\{G^p\}$ is also unsatisfiable, where $G^p=G\setminus\{\neg p\}$.
    
    If $G^p=\square$, then $G=\{\neg p\}$, $\square$ is the resolvent of $G$ and $\{p\}\in T$, and we have a one-step LI-refutation of $T\cup\{G\}$ (this is the base case of the induction). 
    
    Otherwise, we use the induction hypothesis. Note that the formula $T^p$ is satisfiable (by the same assignment as $T$, because it must contain $p$ due to the fact $\{p\}$, so it does not contain $\neg p$) and has fewer variables than $T$. Thus, by the induction hypothesis, there exists an LI-derivation of $\square$ from $T^p\cup\{G^p\}$ starting with $G^p=G\setminus\{\neg p\}$.

    We construct the required LI-refutation of $T\cup\{G\}$ starting with $G$ (similarly to the proof of the Completeness Theorem for Resolution) by adding the literal $\neg p$ to all leaves that are not already in $T\cup\{G\}$ (i.e., they were created by removing $\neg p$), and to all vertices above them. This gives us $T\cup\{G\}\proves_{LI}\neg p$, and finally we add the side clause $\{p\}$ and derive $\square$.
\end{proof}
%todo: another potential frozen bucket, restructure the induction proof?

\begin{example}\label{example:linear-input-resolution}
Consider a (satisfiable, Horn) theory $T$, written in set representation as the formula $T=\{\{p,\neg r,\neg s\},\{\neg q,r\},\{q,\neg s\},\{s\}\}$. Suppose we want to prove that $p\land q$ holds in the theory $T$.\footnote{In Prolog, we would pose the `query': \texttt{?-p,q.}} In the resolution method, we consider the goal $G=\{\neg p,\neg q\}$ and show that $T\cup\{G\}\proves_{LI}\square$. 

Following the guide from the proof, we find a fact in the formula $T$, and perform unit propagation in both $T$ and the goal $G$. We repeat the process until the formula is empty:
\begin{itemize}
    \item $T=\{\{p,\neg r,\neg s\},\{\neg q,r\},\{q,\neg s\},\{s\}\}$, $G=\{\neg p,\neg q\}$
    \item $T^s=\{\{p,\neg r\},\{\neg q,r\},\{q\}\}$, $G^s=\{\neg p,\neg q\}$
    \item $T^{sq}=\{\{p,\neg r\},\{r\}\}$, $G^{sq}=\{\neg p\}$
    \item $T^{sqr}=\{\{p\}\}$, $G^{sqr}=\{\neg p\}$
    \item $T^{sqrp}=\emptyset$, $G^{sqrp}=\square$
\end{itemize}
Now, we construct the resolution refutation in reverse:
\begin{itemize}
    \item $T^{sqrp},G^{sqrp}\proves_{LI}\square$:
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
                        [{\square}]                       ]
        \end{forest} 
    \end{center}
    \item $T^{sqr},G^{sqr}\proves_{LI}\square$:
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
                        [{\square}
                            [,phantom]
                            [{\{\neg p\}}]
                            [{\{p\}}]                        
                        ]
        \end{forest} 
    \end{center}
    
    \item $T^{sq},G^{sq}\proves_{LI}\square$:
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
                    [{\square}
                        [,phantom]
                        [{\{\neg r\}}
                            [,phantom]
                            [{\{\neg p\}}]
                            [{\{p,\neg r\}}]                        
                        ]
                        [{\{r\}}]
                    ]
        \end{forest} 
    \end{center}
    
    \item $T^{s},G^{s}\proves_{LI}\square$:
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
                [{\square}
                    [,phantom]
                    [{\{\neg q\}}
                        [,phantom]
                        [{\{\neg q,\neg r\}}
                            [,phantom]
                            [{\{\neg p,\neg q\}}]
                            [{\{p,\neg r\}}]                        
                        ]
                        [{\{\neg q,r\}}]
                    ]
                    [{\{q\}}]                    
                ]
        \end{forest} 
    \end{center}
    
    \item $T,G\proves_{LI}\square$
    \begin{center}
        \begin{forest}
            for tree={math content,grow=west,text height=2ex, text depth=1ex, l sep=3em}
            [{\square}
                [,phantom]
                [{\{\neg s\}}
                    [,phantom]
                    [{\{\neg q,\neg s\}}
                        [,phantom]
                        [{\{\neg q,\neg r,\neg s\}}
                            [,phantom]
                            [{\{\neg p,\neg q\}}]
                            [{\{p,\neg r,\neg s\}}]                        
                        ]
                        [{\{\neg q,r\}}]
                    ]
                    [{\{q,\neg s\}}]                    
                ]
                [{\{s\}}]
            ]
        \end{forest}
    \end{center}
\end{itemize}
\end{example}

%\section{Rezoluce v Prologu}

\subsection{Prolog Program}

Although the real power of Prolog comes from \emph{unification} and resolution in predicate logic, we will show how Prolog uses the resolution method with a \emph{propositional} program. Adaptation to predicates will be straightforward later.

A \emph{program} in Prolog is a Horn formula containing only \emph{program clauses}, i.e., \emph{facts} or \emph{rules}. A \emph{query} is a conjunction of facts, the negation of the query is the \emph{goal}.

\begin{example}
As an example of a Prolog program, we will use the theory (formula) $T$ and the query $p\land q$ from Example \ref{example:linear-input-resolution}. For instance, the clause $\{p,\neg r,\neg s\}$, which is equivalent to $r\land s\limplies p$, is written in Prolog as: \texttt{p:-r,s.}
\begin{verbatim}
    p:-r,s.
    r:-q.
    q:-s.
    s.    
\end{verbatim}
And we pose the query to the program:
\begin{verbatim}
    ?-p,q.    
\end{verbatim}
\end{example}

\begin{corollary}\label{corollary:propositional-prolog}
    Let $P$ be a program and $Q=p_1\land\dots\land p_n$, and denote $G=\{\neg p_1,\dots,\neg p_n\}$ (i.e., $G\sim \neg Q$). The following conditions are equivalent:
    \begin{itemize}
        \item $P\models Q$,
        \item $P\cup\{G\}$ is unsatisfiable,
        \item $P\cup\{G\}\proves_{LI}\square$, and there is an LI-refutation starting with the goal $G$.
    \end{itemize}
\end{corollary}
\begin{proof}
    The equivalence of the first two conditions is by contradiction, and the equivalence of the second and third is by the Completeness Theorem for LI-Resolution for Horn formulas. (Note that the Program is always satisfiable.)
\end{proof}
