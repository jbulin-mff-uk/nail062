\chapter{Resolution in Predicate Logic} 
\label{chapter:predicate-resolution}

In this chapter, we will discuss how the resolution method introduced in Chapter \ref{chapter:propositional-resolution} can be adapted for predicate logic. This chapter, the last in the part of the lecture about predicate logic, is quite extensive, so let us provide an overview of its structure:

\begin{itemize}
    \item We begin with an informal introduction (Section \ref{section:predicate-resolution-intro}).
\end{itemize}
In the following three sections, we introduce the tools that allow us to deal with the specifics of predicate logic: quantifiers, variables, and terms.
\begin{itemize}
    \item In Section \ref{section:skolemization}, we show how to remove quantifiers using \emph{Skolemization}, to obtain open formulas that can be converted into CNF.
    \item In Section \ref{section:grounding}, we explain that we could seek a resolution refutation `at the propositional logic level' (so-called \emph{grounding}), if we first substituted `suitable' ground terms for the variables.
    \item In Section \ref{section:unification}, we show how to find such `suitable' substitutions using the \emph{unification algorithm}.
\end{itemize}
This will give us all the necessary tools to present the resolution method itself. The rest of the chapter follows a similar structure to Chapter \ref{chapter:propositional-resolution}.
\begin{itemize}
    \item The resolution rule, the resolution proof, and related concepts are described in Section \ref{section:predicate-resolution-method}.
    \item Section \ref{section:predicate-resolution-soundness-completeness} is dedicated to the proof of soundness and completeness.
    \item Finally, in Section \ref{section:predicate-LI-resolution}, we describe LI-resolution and its application in Prolog.
\end{itemize}

\section{Introduction}\label{section:predicate-resolution-intro}

Just as in propositional logic, the resolution method in predicate logic is based on proof by contradiction. To prove that a sentence $\varphi$ holds in a theory $T$ (i.e., $T\models\varphi$), we start with the theory $T\cup\{\neg \varphi\}$. We `convert' this theory to CNF, and then reject the resulting set of clauses $S$ by resolution (i.e., show that $S\proves_R\square$), thereby showing that it is unsatisfiable.

What do we mean by \emph{conjunctive normal form}? The role of a \emph{literal} is played by an \emph{atomic formula}\footnote{I.e., $R(t_1,\dots,t_n)$ or $t_1=t_2$, where $t_i$ are $L$-terms and $R$ is an $n$-ary relation symbol from $L$.} or its negation. A \emph{clause} (in set representation) is a finite set of literals, and a \emph{formula} is a set of clauses.\footnote{As in propositional logic, we also allow infinite sets of clauses.} Otherwise, we use the same terminology, e.g., we talk about \emph{positive}, \emph{negative}, \emph{opposite} literals, $\square$ denotes the empty clause (which is unsatisfiable), etc.

First, let us informally demonstrate the specifics of resolution in predicate logic with a few simple examples.

Notice first that if the theory $T$ and the sentence $\varphi$ are \emph{open} (do not contain quantifiers), we can easily construct a CNF formula $S$ \emph{equivalent} to the theory $T\cup\{\neg \varphi\}$ (i.e., having the same set of models). Even universal quantifiers at the beginning of the formula are not problematic; we can remove them without changing the meaning.\footnote{Any formula is equivalent to its \emph{general closure}, and the equivalence holds in both directions.}

\begin{example}
    Let $T=\{(\forall x)P(x),(\forall x)(P(x)\limplies Q(x))\}$ and $\varphi=(\exists x)Q(x)$. It is easily seen that
    $$
    T\sim \{P(x),P(x)\limplies Q(x)\}\sim\{P(x),\neg P(x)\lor Q(x)\}
    $$ 
    and also:
    $$\neg\varphi=\neg(\exists x)Q(x)\sim(\forall x)\neg Q(x)\sim\neg Q(x)$$ 
    Therefore, we can convert the theory $T\cup\{\neg \varphi\}$ to an \emph{equivalent} CNF formula
    $$
    S = \{\{P(x)\},\{\neg P(x),Q(x)\},\{\neg Q(x)\}\}
    $$
    which we can easily refute by resolution in two steps. (Imagine $P(x)$ as a propositional variable $p$ and $Q(x)$ as a propositional variable $q$.)
\end{example}

Generally, this won't be possible, particularly with existential quantifiers. Unlike in propositional logic, \emph{not} every theory is equivalent to a CNF formula. However, we can always find an \emph{equisatisfiable} CNF formula, i.e., one that is unsatisfiable \emph{if and only if} $T\cup\{\neg \varphi\}$ is unsatisfiable, which is sufficient for proof by contradiction. This construction is called \emph{Skolemization} and involves replacing existentially quantified variables with newly added constant or function symbols.

For example, we replace the formula $(\exists x)\psi(x)$ with the formula $\psi(x/c)$, where $c$ is a new constant symbol representing a \emph{witness}, i.e., an element that satisfies the existential quantifier. Since there may be many such elements, we lose \emph{equivalence} of theories, but it holds that if the original formula is satisfiable, then the new formula is also satisfiable, and vice versa.

\begin{example}
  If $T=\{(\exists x)P(x),P(x)\liff Q(x)\}$ and $\varphi=(\exists x)Q(x)$, then 
  $$
  \neg\varphi\sim(\forall x)\neg Q(x)\sim\neg Q(x)
  $$
  and equivalence can be converted to CNF as usual, giving:
  $$
  T\cup\{\neg \varphi\}\sim\{(\exists x)P(x),\neg P(x)\lor Q(x),\neg Q(x)\lor P(x),\neg Q(x)\}
  $$
  Now we replace the formula $(\exists x)P(x)$ with $P(c)$, where $c$ is a new constant symbol. This gives the CNF formula:
  $$
  S = \{\{P(c)\},\{\neg P(x),Q(x)\},\{\neg Q(x),P(x)\},\{\neg Q(x)\}\}
  $$
  It is not equivalent to the theory $T\cup\{\neg \varphi\}$, but it is \emph{equisatisfiable} (in this case, both are unsatisfiable).
\end{example}

Skolemization can be more complex; sometimes a constant symbol is not enough. If we have a formula of the form $(\forall x)(\exists y)\psi(x,y)$, the chosen witness for $y$ depends on the chosen value for $x$, so `$y$ is a function of $x$'. In this case, we must replace $y$ with $f(x)$, where $f$ is a new unary function symbol. This gives the formula $(\forall x)\psi(x,y/f(x))$, and we can now remove the universal quantifier and write only $\psi(x,y/f(x))$, which is now an open formula, albeit in a different language (extended by the symbol $f$). Skolemization is formally described, and the necessary properties are proved, in Section \ref{section:skolemization}. 

Now let us look at the \emph{resolution rule}. In predicate logic, it is more complex. Again, we will show just a few examples; the formal definition will come later (Section \ref{section:predicate-resolution-method}).

\begin{example}
    In the previous example, we arrived at the following CNF formula $S$, which is unsatisfiable, and we want to refute it by resolution:    
    $$
    S = \{\{P(c)\},\{\neg P(x),Q(x)\},\{\neg Q(x),P(x)\},\{\neg Q(x)\}\}
    $$
    If we look at it `at the propositional logic level' (`ground level') and replace each atomic formula with a new propositional variable, we get $\{\{r\},\{\neg p,q\},\{\neg q,p\},\{\neg q\}\}$, which is not unsatisfiable. We need to use the fact that $P(c)$ and $P(x)$ have a `similar structure' (they are \emph{unifiable}).

    Since the clause $\{\neg P(x),Q(x)\}$ is valid in $S$ (it is an axiom), it is also valid after performing \emph{any substitution}, i.e., the clause $\{\neg P(x/t),Q(x/t)\}$ is a consequence of $S$ for any term $t$. We might imagine `adding' all such clauses to $S$.\footnote{There are infinitely many such clauses; even just the \emph{variants} of one clause, i.e., clauses obtained by simply renaming variables, are infinite in number. We are OK with that; a CNF formula can, by definition, be infinite.} The resulting CNF formula, when converted to the `propositional logic level', would be unsatisfiable.

    The \emph{unification algorithm} directly tells us that the correct substitution is $x/c$, and we include this already in the \emph{resolution rule}, i.e., the \emph{resolvent} of the clauses $\{P(c)\}$ and $\{\neg P(x),Q(x)\}$ will be the clause $\{Q(c)\}$.
\end{example}

Unification can be more complex, and we should also note another difference from propositional logic: we allow resolution over multiple literals at once if they are all \emph{unifiable} together:

\begin{example}
    From the clauses $\{R(x,f(x)),R(g(y),z)\}$ and $\{\neg R(g(c),u),P(u)\}$ (where $R$ is binary relational, $f$ and $g$ are unary functional, and $c$ is a constant symbol), it will be possible to derive the resolvent $\{P(f(g(c))\}$ using the following \emph{substitution} (\emph{unification}): $\{x/g(c),y/c,z/f(g(c)),u/f(g(c))\}$; here we select \emph{both} literals at once from the first clause.
\end{example}

\begin{remark}
    The fact that variables have a `local meaning' in individual clauses (i.e., we can substitute them in one clause without affecting other clauses) is due to the following simple tautology, which holds for any formulas $\psi,\chi$ (even if $x$ is free in both):
    $$
    \models(\forall x)(\psi \land \chi) \leftrightarrow (\forall x)\psi \land (\forall x)\chi
    $$
    
    As we have seen in the previous example, we will also require that clauses in the resolution rule have disjoint sets of variables; this can be achieved by renaming variables, which is a special case of substitution.
\end{remark}

\section{Skolemization}\label{section:skolemization}

In this section, we will describe a procedure to reduce the question of the satisfiability of a given theory $T$ to the question of the satisfiability of an \emph{open} theory $T'$. Recall that $T$ and $T'$ will generally not be equivalent but will be \emph{equisatisfiable}:

\begin{definition}[Equisatisfiability]
Given a theory $T$ in language $L$ and a theory $T'$ in not necessarily the same language $L'$, we say that $T$ and $T'$ are \emph{equisatisfiable} if:
$$
\text{$T$ has a model}\ \Leftrightarrow\ \text{$T'$ has a model}
$$
\end{definition}

The entire construction consists of the following steps, which we will explain below:
\begin{enumerate}
    \item Conversion to \emph{prenex normal form} (pulling quantifiers to the front).
    \item Replacing formulas with their general closures (to obtain sentences).
    \item Removing existential quantifiers (replacing sentences with their \emph{Skolem variants}).
    \item Removing remaining universal quantifiers (resulting in open formulas).
\end{enumerate}

\subsection{Prenex Normal Form}

First, we will show the process by which any formula can have its quantifiers `pulled to the front', i.e., converted to the so-called \emph{prenex normal form}, which starts with a sequence of quantifiers and the rest is a free formula.

\begin{definition}[PNF]
    A formula $\varphi$ is in \emph{prenex normal form (PNF)} if it is of the form
    $$
    (Q_1x_1)\dots(Q_nx_n)\varphi'
    $$
    where $Q_i$ is a quantifier (either $\forall$ or $\exists$), and the formula $\varphi'$ is open. The formula $\varphi'$ is called the \emph{matrix} of $\varphi$, and $(Q_1x_1)\dots(Q_nx_n)$ is the \emph{quantifier prefix}. 
    
    If $\varphi$ is a formula in PNF and all the quantifiers are universal, then we say that $\varphi$ is a \emph{universal} formula.
\end{definition}

The goal of this subsection is to show the following proposition:

\begin{proposition}[Conversion to PNF]\label{proposition:convert-to-pnf}
    For each formula $\varphi$, there exists an equivalent formula in prenex normal form.    
\end{proposition}

The algorithm, like the conversion to CNF, will be based on replacing subformulas with \emph{equivalent} subformulas, aiming to move quantifiers closer to the root of the formula tree. What do we mean by the equivalence of formulas $\varphi\sim\varphi'$? That they have the same meaning, i.e., they have the same truth value in every model and for every variable assignment. Equivalently, that $\models\varphi\liff\varphi'$ holds. We will need the following simple observation:

\begin{observation}\label{observation:pnf-one-step}
    If we replace a subformula $\psi$ of a formula $\varphi$ with an equivalent formula~$\psi'$, then the resulting formula $\varphi'$ is also equivalent to the formula $\varphi$.
\end{observation}

The conversion is based on repeated application of the following syntactic rules:

\begin{lemma}\label{lemma:pnf-conversion-rules}
    Let $\overline{Q}$ denote the quantifier opposite to $Q$. Let $\varphi$ and $\psi$ be formulas, and let $x$ be a variable that is \emph{not} free in the formula $\psi$. Then the following hold:
    \begin{align*}
        \neg (Qx)\varphi\ &\sim\ (\overline{Q}x)\neg\varphi\\
        (Qx)\varphi \land \psi\ &\sim\ (Qx)(\varphi \land \psi)\\
        (Qx)\varphi \lor \psi\ &\sim\ (Qx)(\varphi \lor \psi)\\
        (Qx)\varphi \limplies \psi\ &\sim\ (\overline{Q}x)(\varphi \limplies \psi)\\
        \psi \limplies (Qx)\varphi\ &\sim\ (Qx)(\psi \limplies \varphi)
    \end{align*}
\end{lemma}
\begin{proof}
    The rules can be easily verified semantically, or proved using the tableau method (in that case, if the formulas are not sentences, we must replace them with their general closures).   
\end{proof}

Note that in the rule $(Qx)\varphi \limplies \psi\sim(\overline{Q}x)(\varphi \limplies \psi)$ for pulling a quantifier out of the \emph{antecedent} of an implication, we must change the quantifier (from $\forall$ to $\exists$ and vice versa), whereas when pulling it out of the \emph{consequent}, the quantifier remains the same. Why is it so? This is best seen if we rewrite the implication using disjunction and negation:
$$
(Qx)\varphi \limplies \psi\sim\neg(Qx)\varphi \lor\psi\sim(\overline{Q}x)(\neg\varphi)\lor\psi\sim(\overline{Q}x)(\neg\varphi\lor\psi)\sim (\overline{Q}x)(\varphi \limplies \psi)
$$
Also note the assumption that $x$ is not free in $\psi$. Without it, the rules would not work, e.g.:
$$
(\exists x)P(x)\land Q(x)\ \not\sim\ (\exists x)(P(x)\land Q(x))
$$
In such a situation, we replace the formula with a variant in which we rename the bound variable $x$ to a new variable:
$$
(\exists x)P(x)\land Q(x)\ \sim\ (\exists y)P(y)\land Q(x) \sim (\exists y)(P(y)\land Q(x))
$$
\begin{exercise}
    Prove Observation \ref{observation:pnf-one-step} and all the rules in Lemma \ref{lemma:pnf-conversion-rules}.
\end{exercise}

Let us demonstrate the process with an example:

\begin{example}\label{example:convert-to-pnf}
    Convert the formula $((\forall z)P(x,z)\land P(y,z))\ \limplies\ \neg(\exists x)P(x,y)$ to PNF. We will only write out the intermediate steps. Note what rule was applied to which subformula (and also the renaming of the variable in the first step), and follow the process on the formula tree.
    \begin{align*}
        &\ (\forall z)P(x,z)\land P(y,z)\ \limplies\ \neg(\exists x)P(x,y)\\ \sim &\ 
        (\forall u) P(x,u)\land P(y,z)\ \limplies\ (\forall x)\neg P(x,y)\\ \sim &\ 
        (\forall u)(P(x,u)\land P(y,z))\ \limplies\ (\forall v)\neg P(v,y)\\ \sim &\ 
        (\exists u)( P(x,u)\land P(y,z)\ \limplies\ (\forall v)\neg P(v,y))\\ \sim &\ 
        (\exists u)(\forall v)( P(x,u)\land P(y,z)\ \limplies\ \neg P(v,y))
    \end{align*}    
\end{example}

Now we are ready to prove Proposition \ref{proposition:convert-to-pnf}:

\begin{proof}[Proof of Proposition \ref{proposition:convert-to-pnf}]
    By induction on the structure of the formula $\varphi$, using Lemma \ref{lemma:pnf-conversion-rules} and Observation \ref{observation:pnf-one-step}.
\end{proof}

Since every formula $\varphi(x_1,\dots,x_n)$ is equivalent to its \emph{general closure} $$(\forall x_1)\dots(\forall x_n)\varphi(x_1,\dots,x_n)$$ we can state Proposition \ref{proposition:convert-to-pnf} as follows:

\begin{corollary}
    For every formula $\varphi$, there exists an equivalent \emph{sentence} in PNF.
\end{corollary} 

E.g., in Example \ref{example:convert-to-pnf}, the resulting sentence is $(\forall x)(\forall y)(\forall z)(\exists u)(\forall v)( P(x,u)\land P(y,z)\limplies\neg P(v,y))$.

\begin{remark}
    The prenex form is not unique; the rules for conversion can be applied in different order. As we will see in the next subsection, it is advantageous to first pull out quantifiers that become existential: if we have a choice between $(\forall x)(\exists y)\varphi(x,y)$ and $(\exists y)(\forall x)\varphi(x,y)$, we choose the second option because, in the first case, `$y$ depends on $x$'.
\end{remark}

\subsection{Skolem Variant}

Now we have converted our axioms to equivalent sentences in prenex form. If any sentence contained only universal quantifiers, i.e., if it was of the form 
$$(\forall x_1)\dots(\forall x_n)\varphi(x_1,\dots,x_n)$$ 
where $\varphi$ is open, we could simply replace it with its matrix $\varphi$, which is equivalent in this case. But how do we deal with existential quantifiers, e.g., $(\exists x)\varphi(x)$, $(\forall x)(\exists y)\varphi(x,y)$, etc.? We first replace them with their \emph{Skolem variant}.

\begin{definition}[Skolem Variant]
Let $\varphi$ be an $L$-sentence in PNF, and let all its bound variables be different. Let the existential quantifiers from the prefix of $\varphi$ be $(\exists y_1),\dots,(\exists y_n)$ (in this order), and let for each $i$, $(\forall x_1),\dots,(\forall x_{n_i})$ be precisely all the universal quantifiers preceding the quantifier $(\exists y_i)$ in the prefix of $\varphi$. 

Let $L'$ be the extension of $L$ with \emph{new} $n_i$-ary function symbols $f_1,\dots,f_n$, where the symbol $f_i$ has arity $n_i$ for each $i$. The \emph{Skolem variant} of the sentence $\varphi$ is the $L'$-sentence $\varphi_S$ obtained from $\varphi$ by, for each $i=1,\dots,n$:
\begin{itemize}
    \item removing the quantifier $(\exists y_i)$ from the prefix, and
    \item substituting the variable $y_i$ with the term $f_i(x_1,\dots,x_{n_i})$.
\end{itemize}
This process is also called \emph{skolemization}.
\end{definition}

\begin{example}
    The Skolem variant of the sentence 
    $$\varphi=(\exists y_1)(\forall x_1)(\forall x_2)(\exists y_2)(\forall x_3)R(y_1,x_1,x_2,y_2,x_3)$$
    is the sentence
    $$
    \varphi_S=(\forall x_1)(\forall x_2)(\forall x_3)R(f_1,x_1,x_2,f_2(x_1,x_2),x_3)
    $$
    where $f_1$ is a new constant symbol, and $f_2$ is a new binary function symbol.
\end{example}

\begin{remark}
    Note that when skolemizing, we must start from a sentence! For example, given the formula $(\exists y)E(x,y)$, $E(x,c)$ is \emph{not} its Skolem variant. We must first take the general closure $(\forall x)(\exists y)E(x,y)$, and then correctly skolemize it as $(\forall x)E(x,f(x))$, which is equivalent to the open formula $E(x,f(x))$ (which says something much weaker than $E(x,c)$).

    It is also important that each symbol used in skolemization is genuinely new; its only `role' in the whole theory must be to represent the `existing' elements in this formula.
\end{remark} 

In the following lemma, we show the key property of the Skolem variant:

\begin{lemma}\label{lemma:skolem-variant-conservative-extension}
Let $\varphi=(\forall x_1)\dots(\forall x_n)(\exists y)\psi$ be an $L$-sentence, and let $\varphi'$ be the sentence 
$$
(\forall x_1)\dots(\forall x_n)\psi(y/f(x_1,\dots,x_n))$$
where $f$ is a new function symbol. Then:
\begin{enumerate}[(i)]
    \item The $L$-reduct of every model of $\varphi'$ is a model of $\varphi$, and 
    \item Every model of $\varphi$ can be expanded to a model of $\varphi'$.
\end{enumerate}
\end{lemma}

\begin{proof}
    First, prove part (i): Let $\A'\models\varphi'$, and let $\A$ be its reduct to the language $L$. For each variable assignment $e$, $\A\models\psi[e(y/a)]$ holds for $a=(f(x_1,\dots,x_n))^{\A'}[e]$, so $\A\models\varphi$.
    
    Now, part (ii): Since $\A\models\varphi$, there exists a function $f^A:A^n\to A$ such that for every variable assignment $e$, $\A\models \psi[e(y/a)]$, where $a=f^A(e(x_1),\dots,e(x_n))$. Thus, the expansion of the structure $\A$ obtained by adding the function $f^A$ is a model of $\varphi'$.    
\end{proof}

\begin{remark}
    The expansion of the model in the second part of the lemma need not be (and typically is not) unique, unlike the extension by definition of a new function symbol.
\end{remark}

Applying the previous lemma repeatedly (for all existential quantifiers, in sequence) gives us the following corollary:

\begin{corollary}
    A sentence $\varphi$ and its Skolem variant $\varphi_S$ are equisatisfiable.
\end{corollary}

\subsection{Skolem's Theorem}

In this subsection, we summarize the entire process described in the previous subsections. The key is the following theorem by Norwegian logician Thoralf Skolem:

\begin{theorem}[Skolem's Theorem]
    Every theory has an open conservative extension.
\end{theorem}
\begin{proof}
    Let $T$ be an $L$-theory. Replace each axiom with its general closure (if it is not already a sentence) and convert it to PNF, obtaining an equivalent theory $T'$. Now replace each axiom of the theory $T'$ with its Skolem variant. This gives us a theory $T''$ in an expanded language $L'$. From Lemma \ref{lemma:skolem-variant-conservative-extension}, it follows that the $L$-reduct of each model of $T''$ is a model of $T'$, so $T''$ is an extension of $T'$, and that each model of $T'$ can be expanded to the language $L'$ to be a model of $T''$, so it is a conservative extension. The theory $T''$ is axiomatized by universal sentences; removing the quantifier prefixes (i.e., taking the matrices of the axioms) gives us the open theory $T'''$, which is equivalent to $T''$ and therefore also a conservative extension of~$T$.
\end{proof}

From the semantic characterization of conservative extension, the following corollary easily follows:

\begin{corollary}
    For any theory, using skolemization we can find an equisatisfiable open theory.
\end{corollary}

We can now easily convert an open theory to CNF (expressed as a \emph{formula} $S$ in set representation) using equivalent syntactic transformations, just as in propositional logic (see Section \ref{subsection:convert-to-normal-form}).


\section{Grounding}\label{section:grounding}

In this section, we show that if we have an open theory that is unsatisfiable, we can demonstrate its unsatisfiability `on specific elements'. What do we mean by that? There exists a finite number of \emph{ground instances} of the axioms (instances where we substitute ground terms for the variables) such that their conjunction (which contains no variables) is unsatisfiable.

\begin{definition}[Ground Instance]
    Let $\varphi$ be an open formula in free variables $x_1,\dots,x_n$. We say that the instance $\varphi(x_1/t_1,\dots,x_n/t_n)$ is a \emph{ground instance} if all terms $t_1,\dots,t_n$ are ground (constant).
\end{definition}

\begin{example}
    The theory $T=\{P(x,y)\lor R(x,y),\neg P(c,y),\neg R(x,f(x))\}$ in the language $L=\langle P,R,f,c \rangle$ has no model. We can demonstrate this with the following conjunction of ground instances of the axioms, where we substitute the constant $c$ for the variable $x$ and the ground term $f(c)$ for $y$:
$$
(P(c,f(c))\lor R(c,f(c)))\ \land\ \neg P(c,f(c))\ \land\ \neg R(c,f(c))
$$
\end{example}
This sentence is clearly unsatisfiable. Moreover, the ground atomic sentences $(P(c,f(c))$ and $R(c,f(c))$ can be understood (because they contain no variables) as propositional variables $p_1,p_2$, where $p_1$ means `$P(c,f(c))$ is valid' and $p_2$ means `$R(c,f(c))$ is valid'. We then get the following proposition, which can be easily refuted by resolution:
$$
(p_1 \lor p_2) \land \neg p_1 \land \neg p_2
$$
This process of converting to ground instances (and thus to propositional logic) is called `grounding'. We will formalize it shortly, and prove \emph{Herbrand's theorem},\footnote{French mathematician Jacques Herbrand worked at the end of the 1920s. During his short career (he tragically died at the age of 23), he discovered several other important results, and among other things, formalized the concept of a recursive function.} which states that such an unsatisfiable conjunction of ground instances of the axioms exists for every unsatisfiable theory.

\subsection{Direct Reduction to Propositional Logic}

Let us now realize that thanks to Herbrand's theorem, grounding allows for the following (although inefficient) procedure for refuting formulas by resolution `at the propositional logic level': In the input formula $S$, we replace each clause with the set of all its ground instances (if there are none, i.e., if the language does not contain a constant symbol, we add one constant symbol to the language). In the resulting set of clauses $S'$, we treat atomic sentences as propositional variables, and we refute $S'$ using propositional resolution (which we know is sound and complete).

The problem with this approach is that the number of clauses in $S'$ (ground instances of the clauses from $S$) can be large, even infinite, e.g., whenever the language contains at least one functional (non-constant) symbol.

\begin{example}
If we have a CNF formula $S=\{\{P(x,y),R(x,y)\},\{\neg P(c,y)\},\{\neg R(x,f(x))\}\}$ in the language $L=\langle f,c\rangle$, we replace it with the following infinite formula $S'$:
\begin{align*}
    S'=\{&\{P(c,c),R(c,c)\},\{P(c,f(c)),R(c,f(c))\},\{P(f(c),c),R(f(c),c)\},\dots,\\ 
    &\{\neg P(c,c)\}, \{\neg P(c,f(c))\},\{\neg P(c,f(f(c)))\},\{\neg P(c,f(f(f(c))))\}, \dots,\\
    &\{\neg R(c,f(c))\}, \{\neg R(f(c),f(f(c)))\},\{\neg R(f(f(c)),f(f(f(c))))\},\dots\}    
\end{align*}
It is unsatisfiable because it contains the following finite subset, which is unsatisfiable, as we can easily show by propositional resolution:
$$
\{\{P(c,f(c)),R(c,f(c))\},\{\neg P(c,f(c))\},\{\neg R(c,f(c))\}\}\proves_R\square
$$
\end{example}
In Section \ref{section:unification}, we will show an efficient procedure for finding suitable ground instances of clauses using so-called \emph{unification}.

\subsection{Herbrand's Theorem}

In this subsection, we state and prove Herbrand's theorem. We assume that the language contains some constant symbol: if the language does not contain any, we add one. We need a constant symbol so that there are ground terms and we can create the so-called \emph{Herbrand model}. This is the construction of a semantic object (model) from syntactic objects (ground terms) very similar to the \emph{canonical model} (Definition \ref{definition:canonical-model-predicate}).\footnote{The difference is that we do not add countably many new constant symbols (we only use the constant symbols already in the language), and we do not prescribe how the relations of the model should look.}

\begin{definition}[Herbrand Model]
Let $L=\langle\mathcal R,\mathcal F\rangle$ be a language with at least one constant symbol. An $L$-structure $\A=\langle A,\mathcal R^\A,\mathcal F^\A\rangle$ is a \emph{Herbrand model} if:
\begin{itemize}
    \item $A$ is the set of all ground $L$-terms (the so-called \emph{Herbrand universe}), and
    \item for each $n$-ary function symbol $f\in\mathcal F$ and ground terms $\text{``$t_1$''},\dots,\text{``$t_n$''}\in A$,
    $$
    f^\A(\text{``$t_1$''},\dots,\text{``$t_n$''})=\text{``$f(t_1,\dots,t_n)$''}
    $$
    \item Specifically, for each constant symbol $c\in\mathcal F$, $c^\A=\text{``$c$''}$.
\end{itemize}
We do not impose any conditions on the interpretations of relation symbols.
\end{definition}

Recall that we use quotes around terms informally to clearly distinguish terms as syntactic objects (strings of symbols) from their interpretations (functions).

\begin{example}
Let $L=\langle P,f,c\rangle$ be a language where $P$ is a unary relation symbol, $f$ is a binary function symbol, and $c$ is a constant symbol. The Herbrand universe for this language is the set
$$
A=\{\text{``$c$''},\text{``$f(c,c)$''},\text{``$f(c,f(c,c))$''},\text{``$f(f(c,c),c)$''}\dots\}
$$
The structure $\A=\langle A,P^\A,f^\A,c^\A\rangle$ is a Herbrand model if $c^\A=\text{``$c$''}$ and the function $f^\A$ satisfies:
\begin{itemize}
    \item $f^\A(\text{``$c$''},\text{``$c$''})=\text{``$f(c,c)$''}$,
    \item $f^\A(\text{``$c$''},\text{``$f(c,c)$''})=\text{``$f(c,f(c,c))$''}$,
    \item $f^\A(\text{``$f(c,c)$''},\text{``$c$''})=\text{``$f(f(c,c),c)$''}$, etc.
\end{itemize}
The relation $P^\A$ can be any subset of $A$.
\end{example}

We are now ready to state Herbrand's theorem. Informally, it states that if a theory is satisfiable, i.e., has a model, then it even has a Herbrand model, and otherwise, we can find an unsatisfiable conjunction of ground instances of the axioms, which can be used for resolution refutation `at the propositional logic level'.

\begin{theorem}[Herbrand's Theorem]
Let $T$ be an open theory in a language $L$ without equality and with at least one constant symbol. Then either $T$ has a \emph{Herbrand model}, or there exist finitely many ground instances of axioms of $T$ whose conjunction is unsatisfiable.
\end{theorem}
\begin{proof}
Let $T_\text{ground}$ denote the set of all ground instances of the axioms of the theory $T$. We construct a systematic\footnote{Or any finished tableau, but in such a way that we do not extend contradictory branches.} tableau from the theory $T_\text{ground}$ with the entry $\F\bot$ at the root, but in the language $L$, without expanding it by auxiliary constant symbols to the language $L_C$.\footnote{Since there are no quantifiers in $T_\text{ground}$, auxiliary symbols are not used anywhere in the tableau.}

If the tableau contains a non-contradictory branch, then the canonical model for this branch (again without adding auxiliary constant symbols) is a Herbrand model of $T$. Otherwise, we have a tableau proof of contradiction, so the theory $T_\text{ground}$, and therefore $T$, is unsatisfiable. Since the tableau proof is finite, we used only finitely many ground instances of the axioms $\alpha_\text{ground}\in T_\text{ground}$. Their conjunction is therefore unsatisfiable.
\end{proof}

\begin{remark}
If the language is with equality, we first extend the theory $T$ with the axioms of equality to obtain the theory $T^*$, and if $T^*$ has a Herbrand model $\A$, we factorize it by the congruence $=^A$, just as in the case of the canonical model.
\end{remark}

To conclude this section, we state two corollaries of Herbrand's theorem.

\begin{corollary}
    Let $\varphi(x_1,\dots,x_n)$ be an open formula in a language $L$ with at least one constant symbol. Then there exist ground $L$-terms $t_{ij}$ ($1\leq i\leq m,1\leq j\leq n$) such that the sentence 
    $$
    (\exists x_1)\dots(\exists x_n)\varphi(x_1,\dots,x_n)$$ 
    is true if and only if the following formula (a propositional tautology) is true:
    $$
    \varphi(x_1/t_{11},\dots,x_n/t_{1n})\lor \dots \lor \varphi(x_1/t_{m1},\dots,x_n/t_{mn})
    $$
\end{corollary}
\begin{proof}
The sentence $(\exists x_1)\dots(\exists x_n)\varphi(x_1,\dots,x_n)$ is true if and only if $(\forall x_1)\dots(\forall x_n)\neg\varphi$ is unsatisfiable, i.e., if $\neg\varphi$ is unsatisfiable. The claim follows from Herbrand's theorem applied to the theory $T=\{\neg\varphi\}$.
\end{proof}

\begin{corollary}\label{corollary:herbrands-theorem-corollary-ground}
    Let $T$ be an open theory in a language with at least one constant symbol. The theory $T$ has a model if and only if the theory $T_\text{ground}$, consisting of all ground instances of the axioms of $T$, has a model.
\end{corollary}
\begin{proof}
In a model of the theory $T$, all the axioms hold, and so do all their ground instances. Therefore, it is also a model of $T_\text{ground}$. If $T$ has no model, then by Herbrand's theorem, some finite subset of the theory $T_\text{ground}$ is unsatisfiable.
\end{proof}


\section{Unification}\label{section:unification}

Instead of substituting \emph{all} ground terms and working with this new, huge, and typically infinite set of clauses, it is better to find a substitution `suitable' for the specific resolution step, and work only with it. In this section, we will explain what `suitable' means (so-called \emph{unification}) and how to find it (using the \emph{unification algorithm}).

\subsection{Substitution}

First, let us provide a few examples of `suitable' substitutions:

\begin{example}\label{example:substitutions}
\begin{itemize}
    \item From the clauses $\{P(x),Q(x,a)\}$ and $\{\neg P(y),\neg Q(b,y)\}$, we obtain, using the substitution $\{x/b,y/a\}$, the clauses $\{P(b),Q(b,a)\}$ and $\{\neg P(a),\neg Q(b,a)\}$, and from them, we derive the clause $\{P(b),\neg P(a)\}$ by resolution. We could also use the substitution $\{x/y\}$ and derive the resolvent $\{Q(y,a),\neg Q(b,y)\}$ by resolving over $P(y)$.
    \item Given the clauses $\{P(x),Q(x,a),Q(b,y)\}$ and $\{\neg P(v),\neg Q(u,v)\}$, a suitable substitution is $\{x/b,y/a,u/b,v/a\}$; we get $\{P(b),Q(b,a)\}$ and $\{\neg P(a),\neg Q(b,a)\}$, whose resolvent is $\{P(b),\neg P(a)\}$.
    \item Consider the clauses $\{P(x),Q(x,z)\}$ and $\{\neg P(y),\neg Q(f(y),y)\}$. We could use the substitution $\{x/f(a),y/a,z/a\}$ to obtain the pair of clauses $\{P(f(a)),Q(f(a),a)\}$ and $\{\neg P(a),\neg Q(f(a),a)\}$, resolving to $\{P(f(a)),\neg P(a)\}$.

    However, it is better to use the substitution $\{x/f(z),y/z\}$, after which we have the clauses $\{P(f(z)),Q(f(z),z)\}$ and $\{\neg P(z),\neg Q(f(z),z)\}$. Their resolvent is then the clause $\{P(f(z)),\neg P(z)\}$. This substitution is \emph{more general}; and the resulting resolvent `says more' than $\{P(f(a)),\neg P(a)\}$ (the latter is its consequence, but not vice versa).
\end{itemize}
\end{example}

We now introduce the necessary terminology related to substitutions. Substitutions will be applied to terms or literals (atomic formulas or their negations), collectively referred to as \emph{expressions}.

\begin{definition}[Substitution]
    A \emph{substitution} is a finite set $\sigma=\{x_1/t_1,\dots,x_n/t_n\}$, where $x_i$ are distinct variables and $t_i$ are terms, with the requirement that term $t_i$ is not equal to the variable $x_i$. The substitution $\sigma$ is 
    \begin{itemize}
        \item \emph{ground} if all the terms $t_i$ are ground,
        \item a \emph{renaming of variables} if all terms $t_i$ are distinct variables.
    \end{itemize}
    An \emph{instance} of an expression (term or literal) $E$ \emph{under the substitution $\sigma=\{x_1/t_1,\dots,x_n/t_n\}$} is the expression obtained from $E$ by simultaneously replacing all occurrences of variables $x_i$ with terms $t_i$, denoted $E\sigma$. If $S$ is a set of expressions, we denote $S\sigma=\{E\sigma\mid E\in S\}$.
\end{definition}

Since variables are replaced \emph{simultaneously} for all variables at once, any occurrence of variable $x_i$ in term $t_j$ will not lead to a chain of substitutions.

\begin{example}
For $S=\{P(x),R(y,z)\}$ and substitution $\sigma=\{x/f(y,z),y/x,z/c\}$, we have:
$$
S\sigma=\{P(f(y,z)),R(x,c)\}
$$
\end{example}

Substitutions can be naturally \emph{composed}. The composition of substitutions $\sigma$ and $\tau$, where $\sigma$ is applied first, and then $\tau$ is applied to the result, will be denoted as $\sigma\tau$. Thus, $E(\sigma\tau)=(E\sigma)\tau$ holds for any expression $E$.

\begin{example}\label{example:compose-substitutions}
    Let us start with an example. Given the expression $E=P(x,w,u)$ and the substitutions \begin{align*}
        \sigma&=\{x/f(y),w/v\}\\
        \tau&=\{x/a,y/g(x),v/w,u/c\}
    \end{align*}
    we have $E\sigma=P(f(y),v,u)$ and $(E\sigma)\tau=P(f(g(x)),w,c)$.
    Therefore, it must hold: 
    $$
    \sigma\tau=\{x/f(g(x)),y/g(x),v/w,u/c\}
    $$
\end{example}

Now the formal definition:

\begin{definition}[Composition of Substitutions]
Let $\sigma=\{x_1/t_1,\dots,x_n/t_n\}$ and $\tau=\{y_1/s_1,\dots,y_m/s_m\}$ be substitutions. The \emph{composition of substitutions $\sigma$ and $\tau$} is the substitution
$$
\sigma\tau=\{x_i/t_i\tau\mid x_i\in X,x_i\neq t_i\tau\}\cup\{y_j/s_j\mid y_j\in Y\setminus X\}
$$
where $X=\{x_1,\dots,x_n\}$ and $Y=\{y_1,\dots,y_m\}$.
\end{definition}

Note that the composition of substitutions is not commutative; $\sigma\tau$ is typically a completely different substitution from $\tau\sigma$.

\begin{example}
    If $\sigma$ and $\tau$ are as in Example \ref{example:compose-substitutions}, then: 
    $$
    \tau\sigma=\{x/a,y/g(f(y)),u/c,w/v\}\neq \sigma\tau
    $$
\end{example}

We now show that thus defined composition of substitutions satisfies the required properties and, moreover, that it is \emph{associative}. From associativity, it follows that we do not need to (and will not) write parentheses in the composition $\sigma\tau\varrho$, $\sigma_1\sigma_2\cdots\sigma_n$, etc.

\begin{proposition}
Let $\sigma$, $\tau$, and $\varrho$ be substitutions, and let $E$ be any expression. Then the following holds:
\begin{enumerate}[(i)]
    \item $(E\sigma)\tau=E(\sigma\tau)$
    \item $(\sigma\tau)\varrho=\sigma(\tau\varrho)$
\end{enumerate}
\end{proposition}

\begin{proof}
Let $\sigma=\{x_1/t_1,\dots,x_n/t_n\}$ and $\tau=\{y_1/s_1,\dots,y_m/s_m\}$. It is sufficient to prove the case when the expression $E$ is a single variable, the rest easily follows by structural induction. (Substitutions do not change other symbols.) We split the argument into three cases:
\begin{itemize}
    \item If $E=x_i$ for some $i$, then $E\sigma=t_i$ and $(E\sigma)\tau=t_i\tau=E(\sigma\tau)$, where the second equality is by definition of $\sigma\tau$.
    \item If $E=y_j$ for some $j$, where $y_j\notin\{x_1,\dots,x_n\}$, then $E\sigma=E$ and $(E\sigma)\tau=E\tau=s_j=E(\sigma\tau)$ again by definition of $\sigma\tau$.
    \item If $E$ is another variable, then $(E\sigma)\tau=E=E(\sigma\tau)$.
\end{itemize}
This proves (i). Associativity (ii) can be easily proved by repeated use of (i). The following holds for any expression $E$, hence also for any variable:
$$
E((\sigma\tau)\varrho)=(E(\sigma\tau))\varrho=((E\sigma)\tau)\varrho=(E\sigma)(\tau\varrho)=E(\sigma(\tau\varrho)).
$$
It follows that $(\sigma\tau)\varrho$ and $\sigma(\tau\varrho)$ are the same substitution.\footnote{In more detail: we use the obvious property that for a substitution $\pi$ it holds $\pi=\{z_1/v_1,\dots,z_k/v_k\}$ if and only if $E\pi=v_i$ for $E=z_i$ and $E\pi=E$ if $E$ is a variable different from all $z_i$.}
\end{proof}    

\subsection{Unification Algorithm}

Which substitutions are `suitable'? Those that, after being applied, cause the given expressions to `become the same', i.e., \emph{unified} (see Example \ref{example:substitutions}).

\begin{definition}[Unification]
    Let $S=\{E_1,\dots,E_n\}$ be a finite set of expressions. A substitution $\sigma$ is a \emph{unification of $S$} if $E_1\sigma=E_2\sigma=\cdots =E_n\sigma$, i.e., $S\sigma$ contains a single expression. If it exists, we also say that $S$ is \emph{unifiable}. 
    
    A unification of $S$ is \emph{most general} if for every unification $\tau$ of $S$ there exists a substitution~$\lambda$ such that $\tau=\sigma\lambda$. Note that there may be multiple most general unifications of $S$, but they differ only by renaming of variables.
\end{definition}

\begin{example}
   Consider the set of expressions $S=\{P(f(x),y),P(f(a),w)\}$. The most general unification of $S$ is $\sigma=\{x/a,y/w\}$. Another unification is, for example, $\tau=\{x/a,y/b,w/b\}$, but it is not most general because it cannot yield, for example, the unification $\varrho=\{x/a, y/c, w/c\}$. The unification $\tau$ can be obtained from the most general unification $\sigma$ using the substitution $\lambda=\{w/b\}$: $\tau=\sigma\lambda$.
\end{example}

We now introduce the \emph{unification algorithm}. Its input is a non-empty, finite set of expressions $S$, and its output is either the most general unification of $S$ or information that $S$ is not unifiable. The algorithm proceeds from the start of the expressions, successively applying substitutions to make the expressions more similar. We need the following definition:

Let $p$ be the first (leftmost) position where some two expressions from $S$ differ. Then the \emph{difference in $S$}, denoted $D(S)$, is the set of all subexpressions starting at position $p$ of expressions in $S$.

\begin{example}
For $S=\{P(x,y),P(f(x),z),P(z,f(x))\}$, $p=3$ and $D(S)=\{x,f(x),z\}$.  
\end{example}

\begin{algorithm}[Unification Algorithm]{\,}
\begin{itemize}
    \item \textbf{Input}: a finite set of expressions $S\neq\emptyset$,
    \item \textbf{Output:} the most general unification $\sigma$ of $S$ or information that $S$ is not unifiable
\end{itemize}
\begin{enumerate}[(1)]\setcounter{enumi}{-1}
    \item set $S_0:=S$, $\sigma_0:=\emptyset$, $k:=0$
    \item if $|S_k|=1$, return $\sigma=\sigma_0\sigma_1\cdots \sigma_k$
    \item determine if $D(S_k)$ contains a variable $x$ and a term $t$ \emph{not containing} $x$ 
    \item if yes, set $\sigma_{k+1}:=\{x/t\}$, $S_{k+1}:=S_k\sigma_{k+1}$, $k:=k+1$, and go to (1)
    \item if no, report that $S$ is not unifiable
\end{enumerate}
\end{algorithm}

\begin{remark}
    Finding a variable $x$ and a term $t$ in step (2) can be computationally expensive.
\end{remark}

Before proving correctness, let us demonstrate the algorithm with an example.

\begin{example}
Let us apply the unification algorithm to the following set: 
$$
S=\{P(f(y,g(z)),h(b)),\ P(f(h(w),g(a)),t),\ P(f(h(b),g(z)),y)\}
$$
\begin{itemize}
    \item[($k=0$)] The set $S_0=S$ is not a singleton, $D(S_0)=\{y,h(w),h(b)\}$ contains the term $h(w)$ and the variable $y$ not occurring in $h(w)$. Set $\sigma_1=\{y/h(w)\}$ and $S_1=S_0\sigma_1$, thus we have:
    
    $$S_1=\{P(f(h(w),g(z)),h(b)),\ P(f(h(w),g(a)),t),\ P(f(h(b),g(z)),h(w))\}$$

    \item[($k=1$)] $D(S_1)=\{w,b\}$, $\sigma_2=\{w/b\}$, $S_2=S_1\sigma_2$, thus:
    
    $$S_2=\{P(f(h(b),g(z)),h(b)),\ P(f(h(b),g(a)),t)\}$$
        
    \item[($k=2$)] $D(S_2)=\{z,a\}$, $\sigma_3=\{z/a\}$, $S_3=S_2\sigma_3$, thus:
    
    $$S_3=\{P(f(h(b),g(a)),h(b)),\ P(f(h(b),g(a)),t)\}$$

    \item[($k=3$)] $D(S_3)=\{h(b),t\}$, $\sigma_4=\{t/h(b)\}$, $S_4=S_3\sigma_4$, thus:
    
    $$S_4=\{P(f(h(b),g(a)),h(b))\}$$

    \item[($k=4$)] $S_4$ is one-element, the most general unification of $S$ is the following:
    
    $$
    \sigma=\sigma_1\sigma_2\sigma_3\sigma_4=\{y/h(w)\}\{w/b\}\{z/a\}\{t/h(b)\}=\{y/h(b),w/b,z/a,t/h(b)\}
    $$
\end{itemize}     
\end{example}

\begin{proposition}\label{proposition:unification-algorithm}
The unification algorithm is correct. For any input $S$, it terminates in a finite number of steps, and if $S$ is unifiable, it returns the most general unification $\sigma$; otherwise, it reports that $S$ is not unifiable.

Moreover, if $S$ is unifiable, then for the constructed most general unification $\sigma$, it additionally holds that if $\tau$ is any unification, then $\tau=\sigma\tau$.
\end{proposition}
\begin{proof}
In each step $k$, we eliminate some variable, so the algorithm must terminate. If the algorithm terminates unsuccessfully in step $k$, then it is not possible to unify the set $S_k$. It is easy to see that in that case, it is not possible to unify $S$ either.

If the algorithm returns $\sigma=\sigma_0\sigma_1\cdots\sigma_k$, it is evidently a unification. It remains to prove that it is most general, for which it suffices to prove the stronger property (`moreover') described in the proposition.

Let $\tau$ be any unification of $S$. We will show by induction that for every $0\leq i\leq k$:
$$
\tau=\sigma_0\sigma_1\cdots\sigma_i\tau
$$
For $i=0$, $\sigma_0=\emptyset$ and $\tau=\sigma_0\tau$ holds trivially. Assume it holds for some $i$, and prove it for $i+1$. Let $\sigma_{i+1}=\{x/t\}$. It suffices to prove that for any variable $u$, we have that 
$$
u\sigma_{i+1}\tau=u\tau
$$
From this, $\tau=\sigma_0\sigma_1\cdots\sigma_i\sigma_{i+1}\tau$ immediately follows.

If $u\neq x$, then $u\sigma_{i+1}=u$, so $u\sigma_{i+1}\tau=u\tau$. In the case $u=x$, we have $u\sigma_{i+1}=x\sigma_{i+1}=t$. Since $\tau$ unifies the set $S_i=S\sigma_0\sigma_1\cdots\sigma_i$, and the variable $x$ and the term $t$ are in the difference $D(S_i)$, $\tau$ must unify $x$ and $t$. In other words, $t\tau=x\tau$, i.e., $u\sigma_{i+1}\tau=u\tau$, which is what we wanted to prove.
\end{proof}

\section{Resolution Method}\label{section:predicate-resolution-method}

If we want to prove that $T\models\varphi$, we find (using skolemization) a CNF formula $S$ that is unsatisfiable if and only if the theory $T\cup\{\neg\varphi\}$ is unsatisfiable, i.e., if and only if $T\models\varphi$. Then, all we need to do is to find a resolution refutation of $S$.

In this section, we describe the resolution method itself. Most notions and theorems will be very similar to their counterparts from propositional logic. The only significant difference will be the \emph{resolution rule}.

\subsection{Resolution Rule}

The resolvent of a pair of clauses will be a clause derived from them by applying a \emph{(most general) unification}. First, an example:

\begin{example}
Let $C_1=\{P(x),Q(x,y),Q(x,f(z))\}$ and $C_2=\{\neg P(u),\neg Q(f(u),u)\}$. Select \emph{both} positive literals starting with $Q$ from the first clause, and the negative literal starting with $\neg Q$ from the second clause. The set of expressions $S=\{Q(x,y),Q(x,f(z)),Q(f(u),u)\}$ can be unified using the most general unification $\sigma=\{x/f(f(z)),y/f(z),u/f(z)\}$. After applying this unification, we get the clauses $C_1\sigma=\{P(f(f(z))),Q(f(f(z)),f(z))\}$ and $C_2\sigma=\{\neg P(f(z)),\neg Q(f(f(z)),f(z))\}$, from which we derive the clause $C=\{P(f(f(z))),\neg P(f(z))\}$. This clause will be called the \emph{resolvent} of the original clauses $C_1$ and $C_2$.
\end{example}

\begin{definition}[Resolution Rule]
    Let $C_1$ and $C_2$ be clauses with disjoint sets of variables and let them be of the form
    $$
    C_1=C_1'\sqcup \{A_1,\dots,A_n\},\quad C_2=C_2'\sqcup \{\neg B_1,\dots,\neg B_m\}
    $$
    where $n,m\ge 1$ and the set of expressions $S=\{A_1,\dots,A_n,B_1,\dots,B_m\}$ is unifiable.\footnote{The symbol $\sqcup$ denotes \emph{disjoint union}.} Let $\sigma$ be the most general unification of $S$.\footnote{Recall that unification means that $A_1\sigma=A_2\sigma=\dots=B_1\sigma=\dots=B_m\sigma$.} The \emph{resolvent} of the clauses $C_1$ and $C_2$ is the following clause:
    $$
    C=C_1'\sigma \cup C_2'\sigma
    $$
\end{definition}

\begin{remark}\label{remark:resolution-step-rename}
    The condition about disjoint sets of variables can always be satisfied if we rename the variables in one of the clauses. Why is this necessary? For example, from the clauses $\{\{P(x)\},\{\neg P(f(x))\}\}$ we can obtain the empty clause $\square$ if we replace the clause $\{P(x)\}$ with the clause $\{P(y)\}$. However, the set of expressions $\{P(x),P(f(x))\}$ is not unifiable, so this would not work without renaming variables.
\end{remark}

\subsection{Resolution Proof}

Once we have defined the resolution rule, we can introduce the \emph{resolution proof} and related notions. The definitions will be the same as in propositional logic, with one difference: we allow renaming of variables in the clauses, see Remark \ref{remark:resolution-step-rename}.

\begin{definition}[Resolution Proof]
    A \emph{resolution proof (derivation, deduction)} of a clause~$C$ from a formula $S$ is a \emph{finite} sequence of clauses $C_0,C_1,\dots,C_n=C$
    such that for each $i$,
    \begin{itemize}
        \item either $C_i=C_i'\sigma$ for some clause $C'_i\in S$ and renaming of variables $\sigma$, or
        \item $C_i$ is the resolvent of some $C_j,C_k$ where $j<i$ and $k<i$.
    \end{itemize}
    If a resolution proof exists, we say that $C$ is \emph{provable by resolution} from $S$, and we write $S\proves_R C$. A \emph{resolution refutation} of the formula $S$ is a resolution proof of $\square$ from $S$; in that case, $S$ is \emph{resolution refutable}.
\end{definition}

\begin{remark}
    Why do we need to eliminate multiple literals at once from one clause in the resolution step? Consider the formula $S=\{\{P(x),P(y)\},\{\neg P(x),\neg P(y)\}\}$. It is resolution refutable, but there is no refutation that eliminates only one literal in each step.
\end{remark}

Now we will show an example of application of the resolution method to prove the validity of a sentence.

\begin{example}\label{example:resolution-proof-predicate}
Let $T=\{\neg P(x,x),P(x,y) \limplies P(y,x), P(x,y)\land P(y,z)\to P(x,z)\}$ and let $\varphi$ be the sentence $(\exists x)\neg P(x,f(x))$. We want to show that $T\models\varphi$. The theory $T\cup\{\neg\varphi\}$ is equisatisfiable (in this case, even equivalent) to the following CNF formula:
$$S=\{\{\neg P(x,x)\},\{\neg P(x,y),P(y,x)\},\{\neg P(x,y),\neg P(y,z), P(x,z)\},\{P(x,f(x))\}\}$$
We show that $S\proves_R\square$. The resolution proof is, for example, the following sequence:
\begin{align*}
    &\{\neg P(x,y),\neg P(y,z), P(x,z)\},
    \{P(x',f(x'))\},
    \{\neg P(f(x),z),P(x,z)\},
    \{\neg P(x,y),P(y,x)\},\\
    &\{P(x',f(x'))\},
    \{P(f(x'),x')\},
    \{P(x,x)\},
    \{\neg P(x',x')\},
    \square   
\end{align*}
However, a resolution tree, as shown in Figure \ref{figure:resolution-tree-example}, is more illustrative.
\end{example}

\begin{figure}
\label{figure:resolution-tree-example}
\begin{forest}
    for tree={l=1.5cm, grow=north}
    [{$ \square $}, label=left:{\footnotesize\textcolor{blue}{$x'/x$}}
        [{$ \{\neg P(x',x')\} $}]
        [{$ \{P(x,x)\} $}, label=left:{\footnotesize\textcolor{blue}{$z/x,x'/x$}}
            [{$ \{P(f(x'),x')\} $}, label=right:{\footnotesize\textcolor{blue}{$x/x',y/f(x')$}}
                [{$ \{P(x',f(x'))\} $}]
                [{$ \{\neg P(x,y),P(y,x)\} $}]            
            ]
            [{$ \{\neg P(f(x),z),P(x,z)\} $}, label=left:{\footnotesize\textcolor{blue}{$y/f(x'),x'/x$}}
                [{$ \{P(x',f(x'))\} $}]
                [{$ \{\neg P(x,y),\neg P(y,z), P(x,z) \} $}]                
            ]
        ]
    ]
    \end{forest}
\caption{Resolution refutation of the formula $S$ from Example \ref{example:resolution-proof-predicate}. The unification used in each resolution step is noted.}
\end{figure}


\section{Soundness and Completeness}\label{section:predicate-resolution-soundness-completeness}

In this section, we will prove that the resolution method is both sound and complete in predicate logic, too.

\subsection{Soundness Theorem}

We begin with the proof of soundness of the resolution rule. The principle is the same as the analogous observation in propositional logic. The proof is a bit more technical:

\begin{proposition}[Soundness of the Resolution Step]
Let $C_1$, $C_2$ be clauses, and $C$ their resolvent. If the clauses $C_1$ and $C_2$ are valid in some structure $\A$, then $C$ is also valid in it.
\end{proposition}
\begin{proof}
From the definition of the resolution rule, we know that the clauses and their resolvent can be expressed as $C_1=C_1'\sqcup \{A_1,\dots,A_n\}$, $C_2=C_2'\sqcup \{\neg B_1,\dots,\neg B_m\}$, and $C=C_1'\sigma \cup C_2'\sigma$, where $\sigma$ is the most general unification of the set of expressions $S=\{A_1,\dots,A_n,B_1,\dots,B_m\}$, i.e., $S\sigma=\{A_1\sigma\}$.

Since clauses $C_1$ and $C_2$ are open formulas valid in $\A$, their instances after substitution $\sigma$ are also valid in $\A$, i.e., we have $\A\models C_1\sigma$ and $\A\models C_2\sigma$. We also know that $C_1\sigma=C_1'\sigma \cup \{A_1\sigma\}$ and similarly $C_2\sigma=C_2'\sigma \cup \{\neg A_1\sigma\}$.

Our goal is to show that $\A\models C[e]$ for any variable assignment $e$. If $\A\models A_1\sigma[e]$, then $\A\not\models\neg A_1\sigma[e]$ and it must be that $\A\models C_2'\sigma[e]$. Therefore, $\A\models C[e]$. Conversely, if $\A\not\models A_1\sigma[e]$, then $\A\models C_1'\sigma[e]$, and again $\A\models C[e]$.
\end{proof}

The statement and proof of the Soundness Theorem are now the same as in propositional logic:

\begin{theorem}[Soundness of Resolution]\label{theorem:soundness-of-predicate-resolution}
    If a CNF formula $S$ is resolution refutable, then it is unsatisfiable.
\end{theorem}
\begin{proof}
    We know that $S\proves_R\square$, so let us take some resolution proof of $\square$ from $S$. If there existed a model $\A\models S$, due to the soundness of the resolution rule, we could prove by induction on the length of the proof that $\A\models\square$, which is impossible.
\end{proof}

\subsection{Completeness Theorem}

The completeness theorem for resolution in predicate logic, i.e., that unsatisfiable formulas are resolution refutable, will be proved by reducing to propositional logic. We will show that a resolution proof `at the propositional logic level' can be `lifted' to the predicate logic level.

The key is the following lemma, which provides such `lifting' of one resolution step. Its proof is somewhat technical.

\begin{lemma}[Lifting Lemma]\label{lemma:lifting-lemma}
Let $C_1$ and $C_2$ be clauses with disjoint sets of variables. If $C^*_1$ and $C^*_2$ are ground instances of $C_1$ and $C_2$, respectively, and $C^*$ is a resolvent of $C^*_1$ and $C^*_2$, then there exists a resolvent $C$ of $C_1$ and $C_2$ such that $C^*$ is a ground instance of $C$.
\end{lemma}
\begin{proof}
Let $C^*_1=C_1\tau_1$ and $C^*_2=C_2\tau_2$, where $\tau_1$ and $\tau_2$ are ground substitutions that share no variables. We find a resolvent $C$ such that $C^*=C\tau_1\tau_2$.

Let $C^*$ be a resolvent of $C_1^*$ and $C_2^*$ over the literal $P(t_1,\dots,t_k)$. We know that the clauses $C_1$ and $C_2$ can be expressed as $C_1=C_1' \sqcup \{A_1,\dots,A_n\}$ and $C_2=C_2' \sqcup \{\neg B_1,\dots,\neg B_m\}$, where $\{A_1,\dots,A_n\}\tau_1=\{P(t_1,\dots,t_k)\}$ and $\{\neg B_1,\dots,\neg B_m\}\tau_2=\{\neg P(t_1,\dots,t_k)\}$.

This means that $(\tau_1\tau_2)$ unifies the set of expressions $S=\{A_1,\dots,A_n,B_1,\dots,B_m\}$. Now, take the most general unification $\sigma$ of $S$ obtained from the Unification Algorithm. Define $C$ to be the resolvent $C=C_1'\sigma \cup C_2'\sigma$.

It remains to show that $C^*=C\tau_1\tau_2$. Using the `moreover' property from Proposition \ref{proposition:unification-algorithm} on the correctness of the Unification Algorithm, we know that $(\tau_1\tau_2)=\sigma(\tau_1\tau_2)$, which we use in the third equality of the following calculation. In the fourth equality, we use the fact that $C_1'\tau_1\tau_2=C_1'\tau_1$, and $C_2'\tau_1=C_2'$, which follows from the fact that these are ground substitutions that share no variables, and that $C_1'\tau_1$ and $C_2'\tau_2$ are ground instances:
\begin{align*}
    C\tau_1\tau_2&= (C_1'\sigma \cup C_2'\sigma)\tau_1\tau_2\\
    &=C_1'\sigma\tau_1\tau_2 \cup C_2'\sigma\tau_1\tau_2\\
    &=C_1'\tau_1\tau_2 \cup C_2'\tau_1\tau_2\\
    &=C_1'\tau_1 \cup C_2'\tau_2\\
    &=(C_1\setminus\{A_1,\dots,A_n\})\tau_1\cup (C_2\setminus\{\neg B_1,\dots,B_m\})\tau_2\\
    &=(C_1^*\setminus\{P(t_1,\dots,t_k)\})\cup(C_2^*\setminus \{\neg P(t_1,\dots,t_k)\})=C^*
\end{align*}
\end{proof}

By induction on the length of the resolution proof, we easily obtain the following corollary:

\begin{corollary}\label{corollary:lifting}
Given a CNF formula $S$, let $S^*$ denote the set of all its ground instances. If $S^*\proves_R C^*$ (`at the propositional logic level') for some ground clause $C^*$, then there exists a clause $C$ and a ground substitution $\sigma$ such that $C^*=C\sigma$ and $S\proves_R C$ (`at the predicate logic level').
\end{corollary}

Now it is easy to prove completeness:

\begin{theorem}[Completeness of Resolution]\label{theorem:completeness-of-predicate-resolution}
    If a CNF formula $S$ is unsatisfiable, then it is resolution refutable.
\end{theorem}
\begin{proof}
Let $S^*$ denote the set of all ground instances of clauses from $S$. Since $S$ is unsatisfiable, by Herbrand's theorem (specifically Corollary \ref{corollary:herbrands-theorem-corollary-ground}), $S^*$ is also unsatisfiable. From the completeness theorem for \emph{propositional} resolution, we know that $S^*\proves_R\square$ (`at the propositional logic level'). From the Lifting Lemma (or rather from Corollary \ref{corollary:lifting}), we get a clause $C$ and a ground substitution $\sigma$ such that $C\sigma=\square$ and $S\proves_R C$ (`at the predicate logic level'). But since the empty clause $\square$ is an instance of $C$, $C$ must be the empty clause $\square$ as well. Thus, we have found a resolution refutation $S\proves_R \square$.
\end{proof}


\section{LI-resolution}\label{section:predicate-LI-resolution}

In this section, we recall the concepts of \emph{linear and linear-input proofs}, \emph{LI-resolution}, and its completeness for Horn formulas. The definitions and theorems are the same as in propositional logic (the only difference is that in proofs we can use \emph{variants} of clauses from $S$), and the proof can be done by reduction to propositional logic again using Herbrand's theorem and the Lifting Lemma.

\begin{definition}[Linear and LI Proof]
    A \emph{linear proof} (by resolution) of a clause $C$ from a formula $S$ is a finite sequence
    $$
    \begin{bmatrix}
        C_0 \\
        B_0
    \end{bmatrix},
    \begin{bmatrix}
        C_1 \\
        B_1
    \end{bmatrix},\dots,
    \begin{bmatrix}
        C_n \\
        B_n
    \end{bmatrix},
    C_{n+1}
    $$
    where $C_i$ are called \emph{central} clauses, $C_0$ is the \emph{initial} clause, $C_{n+1}=C$ is the \emph{final} clause, $B_i$ are \emph{side} clauses, and the following hold:
    \begin{itemize}
        \item $C_0$ is a variant of a clause from $S$, for $i\leq n$ $C_{i+1}$ is the resolvent of $C_i$ and $B_i$,
        \item $B_0$ is a variant of a clause from $S$, for $i\leq n$ $B_i$ is a variant of a clause from $S$ or $B_i=C_j$ for some $j<i$. 
    \end{itemize}
    A \emph{linear refutation} of $S$ is a linear proof of $\square$ from $S$.
    
    An \emph{LI-proof} is a linear proof in which each side clause $B_i$ is a variant of a clause from $S$. If there is an LI-proof, we say that $C$ is \emph{LI-provable} from $S$, and write $S\proves_{LI}C$. If $S\proves_{LI}\square$, then $S$ is \emph{LI-refutable}.
\end{definition}

In Remark \ref{remark:linear-resolution}, we noted that `linear' resolution (based on linear proofs) is complete.\footnote{The proof of the harder implication was omitted; it can be found in \emph{A. Nerode, R. Shore: Logic for Applications}~\cite{nerode_logic_2012}.} The same theorem holds for predicate resolution:

\begin{theorem}[Completeness of Linear Resolution]
A clause $C$ has a linear proof from a CNF formula $S$ if and only if it has a resolution proof from $S$ (i.e., $S\proves_R C$).
\end{theorem}
\begin{proof}
From a linear proof, we can easily construct a resolution tree. The converse implication follows from Remark \ref{remark:linear-resolution} and the Lifting Lemma (whose application preserves the linearity of the resolution proof).
\end{proof}

\subsection{Completeness of LI-resolution for Horn Formulas}

Recall the terminology related to Hornness and programs: A \emph{Horn clause} is a clause containing at most one positive literal. A \emph{Horn formula} is a (finite or infinite) set of Horn clauses.
A \emph{fact} is a positive unit (Horn) clause, a \emph{rule} is a (Horn) clause with exactly one positive and at least one negative literal, and a \emph{goal} is a non-empty (Horn) clause with no positive literals. Rules and facts are called \emph{program clauses}.

As in propositional logic, LI-resolution is complete for Horn formulas:

\begin{theorem}[Completeness of LI-resolution for Horn Formulas]\label{theorem:completeness-of-li-resolution-for-horn-predicate}
If a Horn formula $T$ is satisfiable, and $T\cup\{G\}$ is unsatisfiable for some goal $G$, then $T\cup\{G\}\proves_{LI}\square$, by an LI-refutation that starts with the goal $G$.   
\end{theorem}
\begin{proof}
    It follows from the analogous theorem in propositional logic, from Herbrand's theorem, and from the Lifting Lemma.
\end{proof}


\subsection{Resolution in Prolog}\label{subsection:resolution-in-prolog}

Finally, we show the application of LI-resolution in the Prolog programming language. A \emph{program} in Prolog is a Horn formula containing only \emph{program clauses}, i.e., \emph{rules} and \emph{facts}.

\begin{example}\label{example:predicate-prolog-program}
As an example, consider a simple program describing the family relationships of three individuals, described in Table \ref{table:predicate-prolog-program}. On the left side, we see the Prolog syntax, and on the right is the set notation of the corresponding clauses; the corresponding CNF formula will be denoted $P$.

\begin{table}[h]\centering   
    \begin{tabular}{lr}
        \texttt{son(X,Y):-father(Y,X),man(X).} 
        & 
        $\{son(X,Y),\neg father(Y,X),\neg man(X)\}$
        \\
        \texttt{son(X,Y):-mother(Y,X),man(X).}
        &
        $\{son(X,Y),\neg mother(Y,X),\neg man(X)\}$
        \\
        \texttt{man(charlie). }
        &
        $\{man(charlie)\}$
        \\
        \texttt{father(bob,charlie). }
        &
        $\{father(bob,charlie)\}$
        \\
        \texttt{mother(alice,charlie).} 
        &
        $\{mother(alice,charlie)\}$
        \\
        \\
        \texttt{?-son(charlie,X). }
        &
        $\{\neg son(charlie,X)\}$
    \end{tabular}
    \label{table:predicate-prolog-program}
    \caption{Example Prolog Program}    
\end{table} 

The last line in the table is not a part of the program; it is an \emph{existential query}. We are interested to know whether it holds in the program:
$P\models(\exists X)son(charlie,X)$? Note that by negating the query, we obtain the \emph{goal} $G=\{\neg son(charlie,X)\}$. Therefore, we want to \emph{refute} the CNF formula $P\cup\{G\}$. 
\end{example}

As in propositional logic (Corollary \ref{corollary:propositional-prolog}), the completeness of LI-resolution for Horn formulas has the following simple corollary.

\begin{corollary}
For a program $P$ and a goal $G=\{\neg A_1,\dots,\neg A_k\}$ in variables $X_1,\dots,X_n$, the following conditions are equivalent:
\begin{itemize}
    \item $P\models(\exists X_1)\dots(\exists X_n)(A_1\wedge\dots\wedge A_k)$
    \item $P\cup\{G\}$ has an LI-refutation starting with the goal $G$.
\end{itemize}
\end{corollary}
\begin{proof}
It is not hard to see that the program $P$ is always a satisfiable Horn formula. The first condition is equivalent to the unsatisfiability of $P\cup\{G\}$. The equivalence then follows from the completeness of LI-resolution for Horn formulas (Theorem \ref{theorem:completeness-of-li-resolution-for-horn-predicate}).
\end{proof}

If the answer to the query is positive, we want to know the \emph{output substitution} $\sigma$, i.e., the composition of unifications from individual resolution steps, restricted to the variables in $G$. We have that:
    $$
    P\models(A_1\wedge\dots\wedge A_k)\sigma
    $$

\begin{example}
Continuing Example \ref{example:predicate-prolog-program}, we find all output substitutions for our query:

\medskip

\texttt{?-son(charlie,X).}\\
\indent\texttt{X = bob ;}\\
\indent\texttt{X = alice ;}\\
\indent\texttt{No}

\medskip

Which substitution we get depends on which of the two rules we apply to the goal. The respective refutations are shown below. The output substitution is obtained by composing the substitutions from individual steps and restricting it to the variable $X$. (For lack of space, we have abbreviated the constant symbols to $a,b,c$.)

\begin{enumerate}[(a)]
    \item Output substitution $\sigma=\{X/b\}$:

    \hspace{-0.8cm}\begin{forest}
        for tree={math content,grow=west,l sep=5pt}
        [{\square}
            [,phantom]
            [{\{\neg father(X,c)\}}
                [,phantom]
                [{\{\neg father(X,c),\neg man(c)\}}
                    [,phantom]
                    [{\textcolor{red}{\{\neg son(c,X)\}}}]
                    [{\{son(X',Y'),\neg father(Y',X'),\neg man(X')\}}, label=below:{\textcolor{blue}{$\{X'/c,Y'/X\}$}}]
                ]
                [{\{man(c)\}}, label=below:{\textcolor{blue}{$\emptyset$}}]                    
            ]
            [{\{father(b,c)\}}, label=below:{\textcolor{blue}{$\{X/b\}$}}]
        ]
    \end{forest}

    \item Output substitution $\sigma=\{X/a\}$:

    \hspace{-0.8cm}\begin{forest}
        for tree={math content,grow=west,l sep=3pt}
        [{\square}
            [,phantom]
            [{\{\neg mother(X,c)\}}
                [,phantom]
                [{\{\neg mother(X,c),\neg man(c)\}}
                    [,phantom]
                    [{\textcolor{red}{\{\neg son(c,X)\}}}]
                    [{\{son(X',Y'),\neg mother(Y',X'),\neg man(X')\}}, label=below:{\textcolor{blue}{$\{X'/c,Y'/X\}$}}]
                ]
                [{\{man(c)\}}, label=below:{\textcolor{blue}{$\emptyset$}}]                    
            ]
            [{\{mother(a,c)\}}, label=below:{\textcolor{blue}{$\{X/a\}$}}]
        ]
    \end{forest}
\end{enumerate}
    
\end{example}
